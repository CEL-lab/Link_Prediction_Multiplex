{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell #: 1 Library Imports\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import uunet.multinet as ml\n",
    "import itertools\n",
    "import numpy as np  \n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell #: 2 File Paths\n",
    "# File paths for the Excel sheets\n",
    "file_paths = [\n",
    "    \"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Case_1.xlsx\",\n",
    "    \"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Case_2.xlsx\",\n",
    "    \"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Case_3.xlsx\",\n",
    "    \"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Case_4.xlsx\",\n",
    "    \"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Case_5.xlsx\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1 - Removed Edges:\n",
      "      From_Node  From_Layer  To_Node  To_Layer        Flow\n",
      "11         2.0         1.0     12.0       1.0   39.476633\n",
      "16        59.0         1.0     70.0       1.0   65.473726\n",
      "17        12.0         1.0     30.0       1.0   77.406378\n",
      "18        44.0         1.0     30.0       1.0  102.065698\n",
      "19        18.0         1.0     44.0       1.0  111.489735\n",
      "..         ...         ...      ...       ...         ...\n",
      "241      148.0         2.0    159.0       2.0  113.276302\n",
      "242      159.0         2.0    161.0       2.0  113.276302\n",
      "244      125.0         2.0    124.0       2.0  114.968040\n",
      "246      124.0         2.0    136.0       2.0  129.942968\n",
      "247      136.0         2.0    148.0       2.0  129.942968\n",
      "\n",
      "[66 rows x 5 columns] \n",
      "\n",
      "Case 2 - Removed Edges:\n",
      "      From_Node  From_Layer  To_Node  To_Layer       Flow\n",
      "0          3.0         1.0     34.0       1.0   0.000000\n",
      "1          3.0         1.0     34.0       1.0   0.000000\n",
      "2          6.0         1.0     36.0       1.0   0.000000\n",
      "3          9.0         1.0     34.0       1.0   0.000000\n",
      "5         14.0         1.0     71.0       1.0   0.000000\n",
      "..         ...         ...      ...       ...        ...\n",
      "213      200.0         2.0    199.0       2.0  27.551519\n",
      "217      133.0         2.0    130.0       2.0  28.352755\n",
      "218      133.0         2.0    130.0       2.0  28.576103\n",
      "227      174.0         2.0    160.0       2.0  52.799234\n",
      "228      176.0         2.0    160.0       2.0  54.536341\n",
      "\n",
      "[68 rows x 5 columns] \n",
      "\n",
      "Case 3 - Removed Edges:\n",
      "      From_Node  From_Layer  To_Node  To_Layer        Flow\n",
      "4          6.0         1.0     36.0       1.0    0.844136\n",
      "16        59.0         1.0     70.0       1.0   65.473726\n",
      "17        59.0         1.0     55.0       1.0  139.483920\n",
      "19         6.0         1.0      2.0       1.0  160.773154\n",
      "29        13.0         1.0     30.0       1.0  234.179626\n",
      "..         ...         ...      ...       ...         ...\n",
      "257      114.0         2.0    107.0       2.0  177.662455\n",
      "258      107.0         2.0    115.0       2.0  177.662455\n",
      "259      118.0         2.0    125.0       2.0  191.271685\n",
      "261      144.0         2.0    142.0       2.0  209.803391\n",
      "266      115.0         2.0    118.0       2.0  355.549341\n",
      "\n",
      "[63 rows x 5 columns] \n",
      "\n",
      "Case 4 - Removed Edges:\n",
      "      From_Node  From_Layer  To_Node  To_Layer        Flow\n",
      "0          3.0         1.0     34.0       1.0    0.000000\n",
      "1          3.0         1.0     34.0       1.0    0.000000\n",
      "2          6.0         1.0     36.0       1.0    0.000000\n",
      "3          9.0         1.0     34.0       1.0    0.000000\n",
      "7         14.0         1.0     71.0       1.0    0.000000\n",
      "..         ...         ...      ...       ...         ...\n",
      "257      114.0         2.0    107.0       2.0  177.662455\n",
      "258      107.0         2.0    115.0       2.0  177.662455\n",
      "259      118.0         2.0    125.0       2.0  191.271685\n",
      "261      144.0         2.0    142.0       2.0  209.803391\n",
      "266      115.0         2.0    118.0       2.0  355.549341\n",
      "\n",
      "[80 rows x 5 columns] \n",
      "\n",
      "Case 5 - Removed Edges:\n",
      "      From_Node  From_Layer  To_Node  To_Layer        Flow\n",
      "0          6.0         1.0     36.0       1.0    0.000000\n",
      "1          7.0         1.0     33.0       1.0    0.000000\n",
      "2          7.0         1.0     11.0       1.0    0.000000\n",
      "18        59.0         1.0     70.0       1.0   65.473726\n",
      "19        59.0         1.0     55.0       1.0  139.483920\n",
      "..         ...         ...      ...       ...         ...\n",
      "257      144.0         2.0    142.0       2.0  153.527609\n",
      "258      134.0         2.0    123.0       2.0  157.567615\n",
      "259      145.0         2.0    134.0       2.0  157.567615\n",
      "260      154.0         2.0    145.0       2.0  157.567615\n",
      "262      115.0         2.0    118.0       2.0  227.879123\n",
      "\n",
      "[62 rows x 5 columns] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell #: 3 Network Triming Logic\n",
    "# Reading the Excel files\n",
    "cases_data = [pd.read_excel(file_path) for file_path in file_paths]\n",
    "\n",
    "# Function to apply the filtering logic and track removed edges\n",
    "def filter_edges_and_save_removed(cases):\n",
    "    max_flow_per_edge = {}\n",
    "    filtered_cases = []\n",
    "    removed_edges = []\n",
    "\n",
    "    # Step 1: Determine the maximum flow for each edge across all cases\n",
    "    for case in cases:\n",
    "        for _, row in case.iterrows():\n",
    "            edge = (row['From_Node'], row['To_Node'])\n",
    "            flow = row['Flow']\n",
    "            max_flow_per_edge[edge] = max(max_flow_per_edge.get(edge, 0), flow)\n",
    "\n",
    "    # Steps 2 and 3: Filtering edges, setting correct layer, and tracking removed edges\n",
    "    for case_index, case in enumerate(cases, start=1):\n",
    "        filtered_rows = []\n",
    "        removed_edges_rows = []\n",
    "        for _, row in case.iterrows():\n",
    "            edge = (row['From_Node'], row['To_Node'])\n",
    "            if row['Flow'] >= max_flow_per_edge[edge] * 0.9:\n",
    "                row['From_Layer'] = case_index  # Set to correct case number\n",
    "                row['To_Layer'] = case_index    # Set to correct case number\n",
    "                filtered_rows.append(row)\n",
    "            else:\n",
    "                removed_edges_rows.append(row)\n",
    "        filtered_cases.append(pd.DataFrame(filtered_rows))\n",
    "        removed_edges.append(pd.DataFrame(removed_edges_rows))\n",
    "\n",
    "    return filtered_cases, removed_edges\n",
    "\n",
    "# Apply the filtering logic and get removed edges\n",
    "filtered_cases_data, removed_edges_data = filter_edges_and_save_removed(cases_data)\n",
    "\n",
    "# Print the results and optionally save them to files\n",
    "for i, (filtered_case, removed_edges_case) in enumerate(zip(filtered_cases_data, removed_edges_data), 1):\n",
    "    #print(f\"Case {i} - Filtered Data:\\n\", filtered_case.head())\n",
    "    print(f\"Case {i} - Removed Edges:\\n\", removed_edges_case, \"\\n\")\n",
    "    # Optionally, save the filtered cases to files\n",
    "    filtered_case.to_excel(f\"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Filtered_Case_{i}.xlsx\", index=False)\n",
    "    \n",
    "    # Optionally, save the removed edges to files\n",
    "    #removed_edges_case.to_excel(f\"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Removed_Edges_Case_{i}.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 Data:\n",
      "   From_Node  From_Layer  To_Node  To_Layer     Flow\n",
      "0       18.0         1.0     22.0       1.0  0.00000\n",
      "1       25.0         1.0     22.0       1.0  0.00000\n",
      "2       64.0         1.0     26.0       1.0  0.00000\n",
      "3       22.0         1.0     42.0       1.0  0.00000\n",
      "4       19.0         1.0     60.0       1.0  2.28972\n",
      "\n",
      "\n",
      "Layer 2 Data:\n",
      "   From_Node  From_Layer  To_Node  To_Layer      Flow\n",
      "4       22.0         2.0     25.0       2.0  0.000000\n",
      "6       22.0         2.0     42.0       2.0  0.000000\n",
      "7       22.0         2.0     18.0       2.0  0.000000\n",
      "8       64.0         2.0     26.0       2.0  0.000000\n",
      "9       19.0         2.0     60.0       2.0  2.310324\n",
      "\n",
      "\n",
      "Layer 3 Data:\n",
      "   From_Node  From_Layer  To_Node  To_Layer      Flow\n",
      "0       42.0         3.0     22.0       3.0  0.000000\n",
      "1       18.0         3.0     22.0       3.0  0.000000\n",
      "2       64.0         3.0     26.0       3.0  0.000000\n",
      "3       25.0         3.0     22.0       3.0  0.000000\n",
      "5       19.0         3.0     60.0       3.0  2.289642\n",
      "\n",
      "\n",
      "Layer 4 Data:\n",
      "   From_Node  From_Layer  To_Node  To_Layer      Flow\n",
      "4       42.0         4.0     22.0       4.0  0.000000\n",
      "5       18.0         4.0     22.0       4.0  0.000000\n",
      "6       22.0         4.0     25.0       4.0  0.000000\n",
      "8       64.0         4.0     26.0       4.0  0.000000\n",
      "9       19.0         4.0     60.0       4.0  2.310309\n",
      "\n",
      "\n",
      "Layer 5 Data:\n",
      "   From_Node  From_Layer  To_Node  To_Layer     Flow\n",
      "3       42.0         5.0     22.0       5.0  0.00000\n",
      "4       64.0         5.0     26.0       5.0  0.00000\n",
      "5       25.0         5.0     22.0       5.0  0.00000\n",
      "6       22.0         5.0     18.0       5.0  0.00000\n",
      "7       19.0         5.0     60.0       5.0  2.29641\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'filtered_cases_data' is a list of DataFrames\n",
    "for i, layer_data in enumerate(filtered_cases_data):\n",
    "    print(f\"Layer {i+1} Data:\")\n",
    "    print(layer_data.head())  # Prints the first 5 rows of each DataFrame\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell #: 4 Multiplex Network Construction Logic\n",
    "print(\"Initializing multiplex network...\")\n",
    "multiplex_net = ml.empty()\n",
    "\n",
    "# A variable to keep track of the layer index\n",
    "layer_index = 1\n",
    "\n",
    "# Iterate over the filtered data and add each as a layer to the multiplex network\n",
    "for df in filtered_cases_data:\n",
    "    unique_layer_name = f\"Layer_{layer_index}\"\n",
    "    print(f\"\\nAdding layer: {unique_layer_name}\")\n",
    "\n",
    "    # Debug: Print a sample of the DataFrame\n",
    "    print(f\"DataFrame sample from {unique_layer_name}:\\n\", df.head())\n",
    "\n",
    "    # Prepare vertices and add them to the network\n",
    "    vertices = {'actor': list(set(df['From_Node']).union(df['To_Node'])), \n",
    "                'layer': [unique_layer_name] * len(set(df['From_Node']).union(df['To_Node']))}\n",
    "    print(f\"Adding vertices to layer {unique_layer_name}: {vertices['actor']}\")\n",
    "    ml.add_vertices(multiplex_net, vertices)\n",
    "\n",
    "    # Prepare edges and add them to the network\n",
    "    edges = {\n",
    "        'from_actor': df['From_Node'].tolist(),\n",
    "        'from_layer': [unique_layer_name] * len(df),\n",
    "        'to_actor': df['To_Node'].tolist(),\n",
    "        'to_layer': [unique_layer_name] * len(df)\n",
    "    }\n",
    "    print(f\"Adding {len(df)} edges to layer {unique_layer_name}\")\n",
    "    print(f\"Sample edges for layer {unique_layer_name}: {edges['from_actor'][:5]} -> {edges['to_actor'][:5]}\")\n",
    "    ml.add_edges(multiplex_net, edges)\n",
    "\n",
    "    layer_index += 1\n",
    "\n",
    "print(\"\\nNetwork construction completed.\")\n",
    "print(f\"Total layers in network: {len(ml.layers(multiplex_net))}\")\n",
    "\n",
    "print(\"\\nNetwork summary:\")\n",
    "print(ml.summary(multiplex_net))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def find_exclusive_neighbors_for_combination(multiplex_net, layers):\n",
    "    exclusive_neighbors = {}\n",
    "    for layer in layers:\n",
    "        # Retrieve all vertices in the current layer\n",
    "        vertices_info = ml.vertices(multiplex_net, [layer])\n",
    "        actors = vertices_info['actor']  # List of actor IDs in the layer\n",
    "\n",
    "        # Find exclusive neighbors for each actor in the layer\n",
    "        for actor_id in actors:\n",
    "            # Retrieve exclusive neighbors for the actor\n",
    "            neighbors = ml.xneighbors(multiplex_net, actor_id, layers=[layer], mode='all')\n",
    "            if actor_id in exclusive_neighbors:\n",
    "                exclusive_neighbors[actor_id].update(neighbors)\n",
    "            else:\n",
    "                exclusive_neighbors[actor_id] = neighbors\n",
    "\n",
    "    return exclusive_neighbors\n",
    "\n",
    "# Assuming 'multiplex_net' is your multiplex network object\n",
    "all_layers = ml.layers(multiplex_net)\n",
    "\n",
    "# Step 1: Exclusive Neighbors for Individual Layers\n",
    "exclusive_neighbors_individual_layers = {layer: find_exclusive_neighbors_for_combination(multiplex_net, [layer])\n",
    "                                         for layer in all_layers}\n",
    "print(\"Exclusive neighbors for individual layers:\")\n",
    "for layer, neighbors in exclusive_neighbors_individual_layers.items():\n",
    "    print(f\"Layer {layer}: {neighbors}\")\n",
    "\n",
    "# Step 2: Exclusive Neighbors for Two-Layer Combinations\n",
    "two_layer_combinations = list(itertools.combinations(all_layers, 2))\n",
    "exclusive_neighbors_two_layers = {comb: find_exclusive_neighbors_for_combination(multiplex_net, list(comb)) \n",
    "                                  for comb in two_layer_combinations}\n",
    "print(\"\\nExclusive neighbors for two-layer combinations:\")\n",
    "for comb, neighbors in exclusive_neighbors_two_layers.items():\n",
    "    print(f\"Combination {comb}: {neighbors}\")\n",
    "\n",
    "# Step 3: Exclusive Neighbors for Three-Layer Combinations\n",
    "three_layer_combinations = list(itertools.combinations(all_layers, 3))\n",
    "exclusive_neighbors_three_layers = {comb: find_exclusive_neighbors_for_combination(multiplex_net, list(comb)) \n",
    "                                    for comb in three_layer_combinations}\n",
    "print(\"\\nExclusive neighbors for three-layer combinations:\")\n",
    "for comb, neighbors in exclusive_neighbors_three_layers.items():\n",
    "    print(f\"Combination {comb}: {neighbors}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_jaccard_coefficient(G, exclusive_neighbors):\n",
    "    def predict(u, v):\n",
    "        neighbors_u = exclusive_neighbors.get(u, set())\n",
    "        neighbors_v = exclusive_neighbors.get(v, set())\n",
    "        union_size = len(neighbors_u | neighbors_v)\n",
    "        if union_size == 0:\n",
    "            return 0\n",
    "        intersection_size = len(neighbors_u & neighbors_v)\n",
    "        return intersection_size / union_size\n",
    "\n",
    "    return ((u, v, predict(u, v)) for u, v in nx.non_edges(G))\n",
    "\n",
    "def calculate_jaccard_scores_for_layers(nx_graphs, layers, exclusive_neighbors):\n",
    "    combined_graph = nx.Graph()\n",
    "    for layer in layers:\n",
    "        combined_graph = nx.compose(combined_graph, nx_graphs[layer])\n",
    "\n",
    "    jaccard_scores = {}\n",
    "    for u, v, score in modified_jaccard_coefficient(combined_graph, exclusive_neighbors):\n",
    "        if score > 0:\n",
    "            jaccard_scores[f\"{u} - {v}\"] = score\n",
    "    return jaccard_scores\n",
    "\n",
    "# Convert the multiplex network into NetworkX graphs for each layer\n",
    "nx_graphs = ml.to_nx_dict(multiplex_net)\n",
    "\n",
    "# Calculate Jaccard scores for individual layers\n",
    "jaccard_scores_individual_layers = {layer: calculate_jaccard_scores_for_layers(nx_graphs, [layer], exclusive_neighbors_individual_layers[layer])\n",
    "                                    for layer in all_layers}\n",
    "print(\"Jaccard scores for individual layers:\")\n",
    "for layer, scores in jaccard_scores_individual_layers.items():\n",
    "    print(f\"Layer {layer}: {scores}\")\n",
    "\n",
    "# Calculate Jaccard scores for two-layer combinations\n",
    "jaccard_scores_two_layers = {comb: calculate_jaccard_scores_for_layers(nx_graphs, list(comb), exclusive_neighbors_two_layers[comb])\n",
    "                             for comb in two_layer_combinations}\n",
    "print(\"\\nJaccard scores for two-layer combinations:\")\n",
    "for comb, scores in jaccard_scores_two_layers.items():\n",
    "    print(f\"Combination {comb}: {scores}\")\n",
    "\n",
    "# Calculate Jaccard scores for three-layer combinations\n",
    "jaccard_scores_three_layers = {comb: calculate_jaccard_scores_for_layers(nx_graphs, list(comb), exclusive_neighbors_three_layers[comb])\n",
    "                               for comb in three_layer_combinations}\n",
    "print(\"\\nJaccard scores for three-layer combinations:\")\n",
    "for comb, scores in jaccard_scores_three_layers.items():\n",
    "    print(f\"Combination {comb}: {scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_adamic_adar_index(G, exclusive_neighbors):\n",
    "    def predict(u, v):\n",
    "        common_neighbors = exclusive_neighbors.get(u, set()) & exclusive_neighbors.get(v, set())\n",
    "        return sum(1 / log(len(exclusive_neighbors.get(w, set()))) for w in common_neighbors if len(exclusive_neighbors.get(w, set())) > 1)\n",
    "\n",
    "    return ((u, v, predict(u, v)) for u, v in nx.non_edges(G))\n",
    "\n",
    "def calculate_adamic_adar_scores_for_layers(nx_graphs, layers, exclusive_neighbors):\n",
    "    combined_graph = nx.Graph()\n",
    "    for layer in layers:\n",
    "        combined_graph = nx.compose(combined_graph, nx_graphs[layer])\n",
    "\n",
    "    adamic_adar_scores = {}\n",
    "    for u, v, score in modified_adamic_adar_index(combined_graph, exclusive_neighbors):\n",
    "        if score > 0:\n",
    "            adamic_adar_scores[f\"{u} - {v}\"] = score\n",
    "    return adamic_adar_scores\n",
    "\n",
    "# Convert the multiplex network into NetworkX graphs for each layer\n",
    "nx_graphs = ml.to_nx_dict(multiplex_net)\n",
    "\n",
    "# Calculate Adamic-Adar scores for individual layers\n",
    "adamic_adar_scores_individual_layers = {layer: calculate_adamic_adar_scores_for_layers(nx_graphs, [layer], exclusive_neighbors_individual_layers[layer])\n",
    "                                        for layer in all_layers}\n",
    "print(\"Adamic-Adar scores for individual layers:\")\n",
    "for layer, scores in adamic_adar_scores_individual_layers.items():\n",
    "    print(f\"Layer {layer}: {scores}\")\n",
    "\n",
    "# Calculate Adamic-Adar scores for two-layer combinations\n",
    "adamic_adar_scores_two_layers = {comb: calculate_adamic_adar_scores_for_layers(nx_graphs, list(comb), exclusive_neighbors_two_layers[comb])\n",
    "                                 for comb in two_layer_combinations}\n",
    "print(\"\\nAdamic-Adar scores for two-layer combinations:\")\n",
    "for comb, scores in adamic_adar_scores_two_layers.items():\n",
    "    print(f\"Combination {comb}: {scores}\")\n",
    "\n",
    "# Calculate Adamic-Adar scores for three-layer combinations\n",
    "adamic_adar_scores_three_layers = {comb: calculate_adamic_adar_scores_for_layers(nx_graphs, list(comb), exclusive_neighbors_three_layers[comb])\n",
    "                                   for comb in three_layer_combinations}\n",
    "print(\"\\nAdamic-Adar scores for three-layer combinations:\")\n",
    "for comb, scores in adamic_adar_scores_three_layers.items():\n",
    "    print(f\"Combination {comb}: {scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_by_max(scores):\n",
    "    # Flatten all scores into a single list, excluding empty layers or combinations\n",
    "    all_scores = [score for layer_scores in scores.values() for score in layer_scores.values() if layer_scores]\n",
    "\n",
    "    # Check if there are no scores to normalize\n",
    "    if not all_scores:\n",
    "        print(\"No scores to normalize.\")\n",
    "        return {}\n",
    "\n",
    "    # Find the maximum score for normalization\n",
    "    max_score = max(all_scores)\n",
    "\n",
    "    # Normalize the scores\n",
    "    normalized_scores = {}\n",
    "    for key, layer_scores in scores.items():\n",
    "        if layer_scores:  # Check if the layer or combination has scores\n",
    "            normalized_scores[key] = {link: score / max_score for link, score in layer_scores.items()}\n",
    "    \n",
    "    return normalized_scores\n",
    "\n",
    "# Normalize Jaccard and Adamic-Adar scores for individual layers, two-layer combinations, and three-layer combinations\n",
    "normalized_jaccard_scores_individual_layers = normalize_by_max(jaccard_scores_individual_layers)\n",
    "normalized_jaccard_scores_two_layers = normalize_by_max(jaccard_scores_two_layers)\n",
    "normalized_jaccard_scores_three_layers = normalize_by_max(jaccard_scores_three_layers)\n",
    "\n",
    "normalized_adamic_adar_scores_individual_layers = normalize_by_max(adamic_adar_scores_individual_layers)\n",
    "normalized_adamic_adar_scores_two_layers = normalize_by_max(adamic_adar_scores_two_layers)\n",
    "normalized_adamic_adar_scores_three_layers = normalize_by_max(adamic_adar_scores_three_layers)\n",
    "\n",
    "# Display the normalized scores\n",
    "print(\"Normalized Jaccard Scores for Individual Layers:\")\n",
    "for layer, scores in normalized_jaccard_scores_individual_layers.items():\n",
    "    print(f\"{layer}: {scores}\")\n",
    "\n",
    "print(\"\\nNormalized Jaccard Scores for Two-Layer Combinations:\")\n",
    "for combination, scores in normalized_jaccard_scores_two_layers.items():\n",
    "    print(f\"{combination}: {scores}\")\n",
    "\n",
    "print(\"\\nNormalized Jaccard Scores for Three-Layer Combinations:\")\n",
    "for combination, scores in normalized_jaccard_scores_three_layers.items():\n",
    "    print(f\"{combination}: {scores}\")\n",
    "\n",
    "print(\"\\nNormalized Adamic-Adar Scores for Individual Layers:\")\n",
    "for layer, scores in normalized_adamic_adar_scores_individual_layers.items():\n",
    "    print(f\"{layer}: {scores}\")\n",
    "\n",
    "print(\"\\nNormalized Adamic-Adar Scores for Two-Layer Combinations:\")\n",
    "for combination, scores in normalized_adamic_adar_scores_two_layers.items():\n",
    "    print(f\"{combination}: {scores}\")\n",
    "\n",
    "print(\"\\nNormalized Adamic-Adar Scores for Three-Layer Combinations:\")\n",
    "for combination, scores in normalized_adamic_adar_scores_three_layers.items():\n",
    "    print(f\"{combination}: {scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_to_dataframe(normalized_scores, layer_type):\n",
    "    links = []\n",
    "    for layer, layer_scores in normalized_scores.items():\n",
    "        for link, score in layer_scores.items():\n",
    "            node_u, node_v = map(float, link.split(' - '))\n",
    "            if layer_type == \"individual\":\n",
    "                layer_label = layer\n",
    "            else:\n",
    "                # Format the layer label correctly for combinations\n",
    "                layer_label = tuple(layer)\n",
    "            links.append({'Node_U': node_u, 'Node_V': node_v, 'Probability': score, 'Layer': layer_label})\n",
    "    return pd.DataFrame(links)\n",
    "\n",
    "# Convert normalized scores to DataFrames for individual layers, two-layer combinations, and three-layer combinations\n",
    "df_normalized_jaccard_individual = scores_to_dataframe(normalized_jaccard_scores_individual_layers, \"individual\")\n",
    "df_normalized_jaccard_two_layers = scores_to_dataframe(normalized_jaccard_scores_two_layers, \"combination\")\n",
    "df_normalized_jaccard_three_layers = scores_to_dataframe(normalized_jaccard_scores_three_layers, \"combination\")\n",
    "\n",
    "df_normalized_adamic_adar_individual = scores_to_dataframe(normalized_adamic_adar_scores_individual_layers, \"individual\")\n",
    "df_normalized_adamic_adar_two_layers = scores_to_dataframe(normalized_adamic_adar_scores_two_layers, \"combination\")\n",
    "df_normalized_adamic_adar_three_layers = scores_to_dataframe(normalized_adamic_adar_scores_three_layers, \"combination\")\n",
    "# Display the first few rows of each DataFrame\n",
    "print(\"DataFrame with Normalized Jaccard Scores for Individual Layers:\")\n",
    "print(df_normalized_jaccard_individual.head())\n",
    "\n",
    "print(\"\\nDataFrame with Normalized Jaccard Scores for Two-Layer Combinations:\")\n",
    "print(df_normalized_jaccard_two_layers.head())\n",
    "\n",
    "print(\"\\nDataFrame with Normalized Jaccard Scores for Three-Layer Combinations:\")\n",
    "print(df_normalized_jaccard_three_layers.head())\n",
    "\n",
    "print(\"\\nDataFrame with Normalized Adamic-Adar Scores for Individual Layers:\")\n",
    "print(df_normalized_adamic_adar_individual.head())\n",
    "\n",
    "print(\"\\nDataFrame with Normalized Adamic-Adar Scores for Two-Layer Combinations:\")\n",
    "print(df_normalized_adamic_adar_two_layers.head())\n",
    "\n",
    "print(\"\\nDataFrame with Normalized Adamic-Adar Scores for Three-Layer Combinations:\")\n",
    "print(df_normalized_adamic_adar_three_layers.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_flow(node, exclusive_neighbors, layer_data):\n",
    "    total_flow = 0\n",
    "    node_str = str(node)  # Convert node ID to string for key lookup\n",
    "\n",
    "    # Check if the node has exclusive neighbors in the given layer\n",
    "    if node_str not in exclusive_neighbors:\n",
    "        return 0  # Return 0 if node has no exclusive neighbors in this layer\n",
    "\n",
    "    neighbor_count = len(exclusive_neighbors[node_str])\n",
    "    for neighbor_str in exclusive_neighbors[node_str]:\n",
    "        neighbor = float(neighbor_str)  # Convert neighbor ID back to float for comparison\n",
    "\n",
    "        # Extract flow data for edges between the node and its neighbors\n",
    "        flow = layer_data.loc[((layer_data['From_Node'] == node) & (layer_data['To_Node'] == neighbor)) |\n",
    "                              ((layer_data['From_Node'] == neighbor) & (layer_data['To_Node'] == node)), 'Flow']\n",
    "\n",
    "        if not flow.empty:\n",
    "            total_flow += flow.iloc[0]  # Add the flow value to the total\n",
    "\n",
    "    # Calculate average flow if there are neighbors, otherwise return 0\n",
    "    return total_flow / neighbor_count if neighbor_count > 0 else 0\n",
    "\n",
    "def calculate_edge_weight(row, exclusive_neighbors, filtered_cases_data, layer_type):\n",
    "    if layer_type == \"individual\":\n",
    "        layer = row['Layer']\n",
    "        layer_index = int(layer.split('_')[1]) - 1\n",
    "        layer_data = filtered_cases_data[layer_index]\n",
    "        final_avg_flow_u = calculate_average_flow(row['Node_U'], exclusive_neighbors, layer_data)\n",
    "        final_avg_flow_v = calculate_average_flow(row['Node_V'], exclusive_neighbors, layer_data)\n",
    "    else:\n",
    "        # Handle layer combinations directly as a tuple\n",
    "        layers = row['Layer']\n",
    "        avg_flows_u = []\n",
    "        avg_flows_v = []\n",
    "\n",
    "        for layer in layers:\n",
    "            layer_index = int(layer.split('_')[1]) - 1\n",
    "            layer_data = filtered_cases_data[layer_index]\n",
    "\n",
    "            avg_flow_u = calculate_average_flow(row['Node_U'], exclusive_neighbors, layer_data)\n",
    "            avg_flow_v = calculate_average_flow(row['Node_V'], exclusive_neighbors, layer_data)\n",
    "\n",
    "            if avg_flow_u is not None:\n",
    "                avg_flows_u.append(avg_flow_u)\n",
    "            if avg_flow_v is not None:\n",
    "                avg_flows_v.append(avg_flow_v)\n",
    "\n",
    "        final_avg_flow_u = sum(avg_flows_u) / len(avg_flows_u) if avg_flows_u else 0\n",
    "        final_avg_flow_v = sum(avg_flows_v) / len(avg_flows_v) if avg_flows_v else 0\n",
    "\n",
    "    final_weight = (final_avg_flow_u + final_avg_flow_v) / 2 * row['Probability']\n",
    "    return final_weight\n",
    "\n",
    "def find_node_layer(node, exclusive_neighbors):\n",
    "    # Adjust the logic to identify the layer(s) a node belongs to\n",
    "    layers = []\n",
    "    for layer, neighbors in exclusive_neighbors.items():\n",
    "        if str(node) in neighbors:\n",
    "            layers.append(layer)\n",
    "    return layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to apply the calculate_edge_weight function to the DataFrames\n",
    "# Assuming df_normalized_jaccard_individual is your DataFrame for individual layers\n",
    "#df_normalized_jaccard_individual['weight'] = df_normalized_jaccard_individual.apply(\n",
    " #   lambda row: calculate_edge_weight(row, \n",
    " #                                     exclusive_neighbors_individual_layers[row['Layer']], \n",
    " #                                     filtered_cases_data, \n",
    " #                                     \"individual\"),\n",
    "  #  axis=1\n",
    "#)\n",
    "# Now apply the calculate_edge_weight function\n",
    "#df_normalized_jaccard_three_layers['weight'] = df_normalized_jaccard_three_layers.apply(\n",
    " #   lambda row: calculate_edge_weight(row,\n",
    " #                                     exclusive_neighbors_three_layers[row['Layer']],\n",
    "  #                                    filtered_cases_data,\n",
    "  #                                    \"combination\"),\n",
    "   # axis=1\n",
    "#)\n",
    "#print(\"\\nDataFrame with Normalized Jaccard Scores for Three-Layer Combinations:\")\n",
    "#print(df_normalized_jaccard_three_layers.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_format_df(df, exclusive_neighbors, filtered_cases_data, layer_type):\n",
    "    # Apply weight calculation\n",
    "    df['weight'] = df.apply(\n",
    "        lambda row: calculate_edge_weight(row, exclusive_neighbors[row['Layer']], filtered_cases_data, layer_type),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Format for export\n",
    "    formatted_df = df[['Node_U', 'Node_V', 'Layer', 'weight']].copy()\n",
    "    if layer_type != \"individual\":\n",
    "        # For layer combinations, we need to add this link to all layers in the combination\n",
    "        combined_dfs = []\n",
    "        for index, row in formatted_df.iterrows():\n",
    "            for layer in row['Layer']:\n",
    "                combined_dfs.append({'Node_U': row['Node_U'], 'Node_V': row['Node_V'], 'Layer': layer, 'weight': row['weight']})\n",
    "        formatted_df = pd.DataFrame(combined_dfs)\n",
    "    \n",
    "    return formatted_df\n",
    "\n",
    "# Apply the function to all the DataFrames\n",
    "df_jaccard_individual_processed = process_and_format_df(df_normalized_jaccard_individual, exclusive_neighbors_individual_layers, filtered_cases_data, \"individual\")\n",
    "df_jaccard_two_layers_processed = process_and_format_df(df_normalized_jaccard_two_layers, exclusive_neighbors_two_layers, filtered_cases_data, \"combination\")\n",
    "df_jaccard_three_layers_processed = process_and_format_df(df_normalized_jaccard_three_layers, exclusive_neighbors_three_layers, filtered_cases_data, \"combination\")\n",
    "\n",
    "df_adamic_adar_individual_processed = process_and_format_df(df_normalized_adamic_adar_individual, exclusive_neighbors_individual_layers, filtered_cases_data, \"individual\")\n",
    "df_adamic_adar_two_layers_processed = process_and_format_df(df_normalized_adamic_adar_two_layers, exclusive_neighbors_two_layers, filtered_cases_data, \"combination\")\n",
    "df_adamic_adar_three_layers_processed = process_and_format_df(df_normalized_adamic_adar_three_layers, exclusive_neighbors_three_layers, filtered_cases_data, \"combination\")\n",
    "\n",
    "# Combine all processed DataFrames\n",
    "combined_df = pd.concat([\n",
    "    df_jaccard_individual_processed,\n",
    "    df_jaccard_two_layers_processed,\n",
    "    df_jaccard_three_layers_processed,\n",
    "    df_adamic_adar_individual_processed,\n",
    "    df_adamic_adar_two_layers_processed,\n",
    "    df_adamic_adar_three_layers_processed\n",
    "])\n",
    "\n",
    "# Export the combined DataFrame to a CSV file\n",
    "#combined_df.to_csv('/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/processed_network_data.csv', index=False)\n",
    "\n",
    "def export_processed_data(df, exclusive_neighbors, filtered_cases_data, layer_type, filename_prefix):\n",
    "    processed_df = process_and_format_df(df, exclusive_neighbors, filtered_cases_data, layer_type)\n",
    "    file_path = f'/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/{filename_prefix}_{layer_type}.csv'\n",
    "    processed_df.to_csv(file_path, index=False)\n",
    "    return file_path\n",
    "\n",
    "# Exporting each DataFrame to a separate CSV file\n",
    "file_jaccard_individual = export_processed_data(df_normalized_jaccard_individual, exclusive_neighbors_individual_layers, filtered_cases_data, \"individual\", \"jaccard\")\n",
    "file_jaccard_two_layers = export_processed_data(df_normalized_jaccard_two_layers, exclusive_neighbors_two_layers, filtered_cases_data, \"two_layers\", \"jaccard\")\n",
    "file_jaccard_three_layers = export_processed_data(df_normalized_jaccard_three_layers, exclusive_neighbors_three_layers, filtered_cases_data, \"three_layers\", \"jaccard\")\n",
    "\n",
    "file_adamic_adar_individual = export_processed_data(df_normalized_adamic_adar_individual, exclusive_neighbors_individual_layers, filtered_cases_data, \"individual\", \"adamic_adar\")\n",
    "file_adamic_adar_two_layers = export_processed_data(df_normalized_adamic_adar_two_layers, exclusive_neighbors_two_layers, filtered_cases_data, \"two_layers\", \"adamic_adar\")\n",
    "file_adamic_adar_three_layers = export_processed_data(df_normalized_adamic_adar_three_layers, exclusive_neighbors_three_layers, filtered_cases_data, \"three_layers\", \"adamic_adar\")\n",
    "\n",
    "# Returning the file paths for download\n",
    "[file_jaccard_individual, file_jaccard_two_layers, file_jaccard_three_layers, \n",
    " file_adamic_adar_individual, file_adamic_adar_two_layers, file_adamic_adar_three_layers]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
