{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell #: 1 Library Imports\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import uunet.multinet as ml\n",
    "import itertools\n",
    "import numpy as np  \n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell #: 2 File Paths\n",
    "# File paths for the Excel sheets\n",
    "file_paths = [\n",
    "    \"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Case_1.xlsx\",\n",
    "    \"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Case_2.xlsx\",\n",
    "    \"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Case_3.xlsx\",\n",
    "    \"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Case_4.xlsx\",\n",
    "    \"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Case_5.xlsx\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1 - Removed Edges:\n",
      "      From_Node  From_Layer  To_Node  To_Layer        Flow\n",
      "11         2.0         1.0     12.0       1.0   39.476633\n",
      "16        59.0         1.0     70.0       1.0   65.473726\n",
      "17        12.0         1.0     30.0       1.0   77.406378\n",
      "18        44.0         1.0     30.0       1.0  102.065698\n",
      "19        18.0         1.0     44.0       1.0  111.489735\n",
      "..         ...         ...      ...       ...         ...\n",
      "241      148.0         2.0    159.0       2.0  113.276302\n",
      "242      159.0         2.0    161.0       2.0  113.276302\n",
      "244      125.0         2.0    124.0       2.0  114.968040\n",
      "246      124.0         2.0    136.0       2.0  129.942968\n",
      "247      136.0         2.0    148.0       2.0  129.942968\n",
      "\n",
      "[66 rows x 5 columns] \n",
      "\n",
      "Case 2 - Removed Edges:\n",
      "      From_Node  From_Layer  To_Node  To_Layer       Flow\n",
      "0          3.0         1.0     34.0       1.0   0.000000\n",
      "1          3.0         1.0     34.0       1.0   0.000000\n",
      "2          6.0         1.0     36.0       1.0   0.000000\n",
      "3          9.0         1.0     34.0       1.0   0.000000\n",
      "5         14.0         1.0     71.0       1.0   0.000000\n",
      "..         ...         ...      ...       ...        ...\n",
      "213      200.0         2.0    199.0       2.0  27.551519\n",
      "217      133.0         2.0    130.0       2.0  28.352755\n",
      "218      133.0         2.0    130.0       2.0  28.576103\n",
      "227      174.0         2.0    160.0       2.0  52.799234\n",
      "228      176.0         2.0    160.0       2.0  54.536341\n",
      "\n",
      "[68 rows x 5 columns] \n",
      "\n",
      "Case 3 - Removed Edges:\n",
      "      From_Node  From_Layer  To_Node  To_Layer        Flow\n",
      "4          6.0         1.0     36.0       1.0    0.844136\n",
      "16        59.0         1.0     70.0       1.0   65.473726\n",
      "17        59.0         1.0     55.0       1.0  139.483920\n",
      "19         6.0         1.0      2.0       1.0  160.773154\n",
      "29        13.0         1.0     30.0       1.0  234.179626\n",
      "..         ...         ...      ...       ...         ...\n",
      "257      114.0         2.0    107.0       2.0  177.662455\n",
      "258      107.0         2.0    115.0       2.0  177.662455\n",
      "259      118.0         2.0    125.0       2.0  191.271685\n",
      "261      144.0         2.0    142.0       2.0  209.803391\n",
      "266      115.0         2.0    118.0       2.0  355.549341\n",
      "\n",
      "[63 rows x 5 columns] \n",
      "\n",
      "Case 4 - Removed Edges:\n",
      "      From_Node  From_Layer  To_Node  To_Layer        Flow\n",
      "0          3.0         1.0     34.0       1.0    0.000000\n",
      "1          3.0         1.0     34.0       1.0    0.000000\n",
      "2          6.0         1.0     36.0       1.0    0.000000\n",
      "3          9.0         1.0     34.0       1.0    0.000000\n",
      "7         14.0         1.0     71.0       1.0    0.000000\n",
      "..         ...         ...      ...       ...         ...\n",
      "257      114.0         2.0    107.0       2.0  177.662455\n",
      "258      107.0         2.0    115.0       2.0  177.662455\n",
      "259      118.0         2.0    125.0       2.0  191.271685\n",
      "261      144.0         2.0    142.0       2.0  209.803391\n",
      "266      115.0         2.0    118.0       2.0  355.549341\n",
      "\n",
      "[80 rows x 5 columns] \n",
      "\n",
      "Case 5 - Removed Edges:\n",
      "      From_Node  From_Layer  To_Node  To_Layer        Flow\n",
      "0          6.0         1.0     36.0       1.0    0.000000\n",
      "1          7.0         1.0     33.0       1.0    0.000000\n",
      "2          7.0         1.0     11.0       1.0    0.000000\n",
      "18        59.0         1.0     70.0       1.0   65.473726\n",
      "19        59.0         1.0     55.0       1.0  139.483920\n",
      "..         ...         ...      ...       ...         ...\n",
      "257      144.0         2.0    142.0       2.0  153.527609\n",
      "258      134.0         2.0    123.0       2.0  157.567615\n",
      "259      145.0         2.0    134.0       2.0  157.567615\n",
      "260      154.0         2.0    145.0       2.0  157.567615\n",
      "262      115.0         2.0    118.0       2.0  227.879123\n",
      "\n",
      "[62 rows x 5 columns] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell #: 3 Network Triming Logic\n",
    "# Reading the Excel files\n",
    "cases_data = [pd.read_excel(file_path) for file_path in file_paths]\n",
    "\n",
    "# Function to apply the filtering logic and track removed edges\n",
    "def filter_edges_and_save_removed(cases):\n",
    "    max_flow_per_edge = {}\n",
    "    filtered_cases = []\n",
    "    removed_edges = []\n",
    "\n",
    "    # Step 1: Determine the maximum flow for each edge across all cases\n",
    "    for case in cases:\n",
    "        for _, row in case.iterrows():\n",
    "            edge = (row['From_Node'], row['To_Node'])\n",
    "            flow = row['Flow']\n",
    "            max_flow_per_edge[edge] = max(max_flow_per_edge.get(edge, 0), flow)\n",
    "\n",
    "    # Steps 2 and 3: Filtering edges, setting correct layer, and tracking removed edges\n",
    "    for case_index, case in enumerate(cases, start=1):\n",
    "        filtered_rows = []\n",
    "        removed_edges_rows = []\n",
    "        for _, row in case.iterrows():\n",
    "            edge = (row['From_Node'], row['To_Node'])\n",
    "            if row['Flow'] >= max_flow_per_edge[edge] * 0.9:\n",
    "                row['From_Layer'] = case_index  # Set to correct case number\n",
    "                row['To_Layer'] = case_index    # Set to correct case number\n",
    "                filtered_rows.append(row)\n",
    "            else:\n",
    "                removed_edges_rows.append(row)\n",
    "        filtered_cases.append(pd.DataFrame(filtered_rows))\n",
    "        removed_edges.append(pd.DataFrame(removed_edges_rows))\n",
    "\n",
    "    return filtered_cases, removed_edges\n",
    "\n",
    "# Apply the filtering logic and get removed edges\n",
    "filtered_cases_data, removed_edges_data = filter_edges_and_save_removed(cases_data)\n",
    "\n",
    "# Print the results and optionally save them to files\n",
    "for i, (filtered_case, removed_edges_case) in enumerate(zip(filtered_cases_data, removed_edges_data), 1):\n",
    "    #print(f\"Case {i} - Filtered Data:\\n\", filtered_case.head())\n",
    "    print(f\"Case {i} - Removed Edges:\\n\", removed_edges_case, \"\\n\")\n",
    "    # Optionally, save the filtered cases to files\n",
    "    filtered_case.to_excel(f\"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Filtered_Case_{i}.xlsx\", index=False)\n",
    "    \n",
    "    # Optionally, save the removed edges to files\n",
    "    #removed_edges_case.to_excel(f\"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Removed_Edges_Case_{i}.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing multiplex network...\n",
      "\n",
      "Adding layer: Layer_1\n",
      "DataFrame sample from Layer_1:\n",
      "    From_Node  From_Layer  To_Node  To_Layer     Flow\n",
      "0       18.0         1.0     22.0       1.0  0.00000\n",
      "1       25.0         1.0     22.0       1.0  0.00000\n",
      "2       64.0         1.0     26.0       1.0  0.00000\n",
      "3       22.0         1.0     42.0       1.0  0.00000\n",
      "4       19.0         1.0     60.0       1.0  2.28972\n",
      "Adding vertices to layer Layer_1: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 40.0, 41.0, 42.0, 43.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 56.0, 57.0, 58.0, 59.0, 60.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 71.0, 72.0, 73.0, 74.0, 76.0, 78.0, 79.0, 81.0, 82.0, 85.0, 86.0, 87.0, 88.0, 90.0, 91.0, 93.0, 95.0, 98.0, 100.0, 101.0, 104.0, 105.0, 106.0, 107.0, 109.0, 111.0, 112.0, 113.0, 114.0, 115.0, 117.0, 118.0, 120.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 138.0, 139.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0]\n",
      "Adding 201 edges to layer Layer_1\n",
      "Sample edges for layer Layer_1: [18.0, 25.0, 64.0, 22.0, 19.0] -> [22.0, 22.0, 26.0, 42.0, 60.0]\n",
      "\n",
      "Adding layer: Layer_2\n",
      "DataFrame sample from Layer_2:\n",
      "    From_Node  From_Layer  To_Node  To_Layer      Flow\n",
      "4       22.0         2.0     25.0       2.0  0.000000\n",
      "6       22.0         2.0     42.0       2.0  0.000000\n",
      "7       22.0         2.0     18.0       2.0  0.000000\n",
      "8       64.0         2.0     26.0       2.0  0.000000\n",
      "9       19.0         2.0     60.0       2.0  2.310324\n",
      "Adding vertices to layer Layer_2: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 8.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 21.0, 22.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 35.0, 37.0, 38.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 58.0, 59.0, 60.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 73.0, 74.0, 76.0, 78.0, 79.0, 81.0, 82.0, 85.0, 86.0, 87.0, 88.0, 90.0, 91.0, 93.0, 95.0, 98.0, 100.0, 101.0, 104.0, 105.0, 106.0, 107.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 175.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0]\n",
      "Adding 199 edges to layer Layer_2\n",
      "Sample edges for layer Layer_2: [22.0, 22.0, 22.0, 64.0, 19.0] -> [25.0, 42.0, 18.0, 26.0, 60.0]\n",
      "\n",
      "Adding layer: Layer_3\n",
      "DataFrame sample from Layer_3:\n",
      "    From_Node  From_Layer  To_Node  To_Layer      Flow\n",
      "0       42.0         3.0     22.0       3.0  0.000000\n",
      "1       18.0         3.0     22.0       3.0  0.000000\n",
      "2       64.0         3.0     26.0       3.0  0.000000\n",
      "3       25.0         3.0     22.0       3.0  0.000000\n",
      "5       19.0         3.0     60.0       3.0  2.289642\n",
      "Adding vertices to layer Layer_3: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 37.0, 38.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 71.0, 72.0, 73.0, 74.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 106.0, 108.0, 109.0, 111.0, 113.0, 116.0, 117.0, 120.0, 123.0, 124.0, 125.0, 126.0, 127.0, 131.0, 134.0, 135.0, 136.0, 138.0, 139.0, 142.0, 143.0, 144.0, 145.0, 146.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 170.0, 171.0, 172.0, 173.0, 175.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0]\n",
      "Adding 204 edges to layer Layer_3\n",
      "Sample edges for layer Layer_3: [42.0, 18.0, 64.0, 25.0, 19.0] -> [22.0, 22.0, 26.0, 22.0, 60.0]\n",
      "\n",
      "Adding layer: Layer_4\n",
      "DataFrame sample from Layer_4:\n",
      "    From_Node  From_Layer  To_Node  To_Layer      Flow\n",
      "4       42.0         4.0     22.0       4.0  0.000000\n",
      "5       18.0         4.0     22.0       4.0  0.000000\n",
      "6       22.0         4.0     25.0       4.0  0.000000\n",
      "8       64.0         4.0     26.0       4.0  0.000000\n",
      "9       19.0         4.0     60.0       4.0  2.310309\n",
      "Adding vertices to layer Layer_4: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 8.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 21.0, 22.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 35.0, 37.0, 38.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 58.0, 59.0, 60.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 73.0, 74.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 106.0, 108.0, 109.0, 111.0, 113.0, 116.0, 117.0, 120.0, 123.0, 124.0, 125.0, 126.0, 127.0, 131.0, 134.0, 135.0, 136.0, 138.0, 139.0, 142.0, 143.0, 144.0, 145.0, 146.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 170.0, 171.0, 172.0, 173.0, 175.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0]\n",
      "Adding 187 edges to layer Layer_4\n",
      "Sample edges for layer Layer_4: [42.0, 18.0, 22.0, 64.0, 19.0] -> [22.0, 22.0, 25.0, 26.0, 60.0]\n",
      "\n",
      "Adding layer: Layer_5\n",
      "DataFrame sample from Layer_5:\n",
      "    From_Node  From_Layer  To_Node  To_Layer     Flow\n",
      "3       42.0         5.0     22.0       5.0  0.00000\n",
      "4       64.0         5.0     26.0       5.0  0.00000\n",
      "5       25.0         5.0     22.0       5.0  0.00000\n",
      "6       22.0         5.0     18.0       5.0  0.00000\n",
      "7       19.0         5.0     60.0       5.0  2.29641\n",
      "Adding vertices to layer Layer_5: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 108.0, 109.0, 110.0, 111.0, 113.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 133.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 143.0, 144.0, 146.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 172.0, 173.0, 175.0, 177.0, 178.0, 179.0, 180.0, 182.0, 183.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 197.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0]\n",
      "Adding 205 edges to layer Layer_5\n",
      "Sample edges for layer Layer_5: [42.0, 64.0, 25.0, 22.0, 19.0] -> [22.0, 26.0, 22.0, 18.0, 60.0]\n",
      "\n",
      "Network construction completed.\n",
      "Total layers in network: 5\n",
      "\n",
      "Network summary:\n",
      "{'layer': ['Layer_1', 'Layer_2', 'Layer_4', 'Layer_5', 'Layer_3'], 'n': [188, 189, 185, 196, 192], 'm': [188, 186, 179, 195, 195], 'dir': [False, False, False, False, False], 'nc': [12, 14, 17, 14, 12], 'slc': [59, 106, 44, 54, 48], 'dens': [0.0106951871657754, 0.010469436001350895, 0.010517038777908343, 0.01020408163265306, 0.010634816753926702], 'cc': [0.060810810810810814, 0.034220532319391636, 0.06072874493927125, 0.06818181818181818, 0.07894736842105263], 'apl': [6.458211572180011, 12.241868823000898, 7.449260042283298, 6.146051712089448, 6.084219858156028], 'dia': [14, 32, 18, 15, 15]}\n"
     ]
    }
   ],
   "source": [
    "#Cell #: 4 Multiplex Network Construction Logic\n",
    "print(\"Initializing multiplex network...\")\n",
    "multiplex_net = ml.empty()\n",
    "\n",
    "# A variable to keep track of the layer index\n",
    "layer_index = 1\n",
    "\n",
    "# Iterate over the filtered data and add each as a layer to the multiplex network\n",
    "for df in filtered_cases_data:\n",
    "    unique_layer_name = f\"Layer_{layer_index}\"\n",
    "    print(f\"\\nAdding layer: {unique_layer_name}\")\n",
    "\n",
    "    # Debug: Print a sample of the DataFrame\n",
    "    print(f\"DataFrame sample from {unique_layer_name}:\\n\", df.head())\n",
    "\n",
    "    # Prepare vertices and add them to the network\n",
    "    vertices = {'actor': list(set(df['From_Node']).union(df['To_Node'])), \n",
    "                'layer': [unique_layer_name] * len(set(df['From_Node']).union(df['To_Node']))}\n",
    "    print(f\"Adding vertices to layer {unique_layer_name}: {vertices['actor']}\")\n",
    "    ml.add_vertices(multiplex_net, vertices)\n",
    "\n",
    "    # Prepare edges and add them to the network\n",
    "    edges = {\n",
    "        'from_actor': df['From_Node'].tolist(),\n",
    "        'from_layer': [unique_layer_name] * len(df),\n",
    "        'to_actor': df['To_Node'].tolist(),\n",
    "        'to_layer': [unique_layer_name] * len(df)\n",
    "    }\n",
    "    print(f\"Adding {len(df)} edges to layer {unique_layer_name}\")\n",
    "    print(f\"Sample edges for layer {unique_layer_name}: {edges['from_actor'][:5]} -> {edges['to_actor'][:5]}\")\n",
    "    ml.add_edges(multiplex_net, edges)\n",
    "\n",
    "    layer_index += 1\n",
    "\n",
    "print(\"\\nNetwork construction completed.\")\n",
    "print(f\"Total layers in network: {len(ml.layers(multiplex_net))}\")\n",
    "\n",
    "print(\"\\nNetwork summary:\")\n",
    "print(ml.summary(multiplex_net))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def find_exclusive_neighbors_for_combination(multiplex_net, layers):\n",
    "    exclusive_neighbors = {}\n",
    "    for layer in layers:\n",
    "        # Retrieve all vertices in the current layer\n",
    "        vertices_info = ml.vertices(multiplex_net, [layer])\n",
    "        actors = vertices_info['actor']  # List of actor IDs in the layer\n",
    "\n",
    "        # Find exclusive neighbors for each actor in the layer\n",
    "        for actor_id in actors:\n",
    "            # Retrieve exclusive neighbors for the actor\n",
    "            neighbors = ml.xneighbors(multiplex_net, actor_id, layers=[layer], mode='all')\n",
    "            if actor_id in exclusive_neighbors:\n",
    "                exclusive_neighbors[actor_id].update(neighbors)\n",
    "            else:\n",
    "                exclusive_neighbors[actor_id] = neighbors\n",
    "\n",
    "    return exclusive_neighbors\n",
    "\n",
    "# Assuming 'multiplex_net' is your multiplex network object\n",
    "all_layers = ml.layers(multiplex_net)\n",
    "\n",
    "# Step 1: Exclusive Neighbors for Individual Layers\n",
    "exclusive_neighbors_individual_layers = {layer: find_exclusive_neighbors_for_combination(multiplex_net, [layer])\n",
    "                                         for layer in all_layers}\n",
    "print(\"Exclusive neighbors for individual layers:\")\n",
    "for layer, neighbors in exclusive_neighbors_individual_layers.items():\n",
    "    print(f\"Layer {layer}: {neighbors}\")\n",
    "\n",
    "# Step 2: Exclusive Neighbors for Two-Layer Combinations\n",
    "two_layer_combinations = list(itertools.combinations(all_layers, 2))\n",
    "exclusive_neighbors_two_layers = {comb: find_exclusive_neighbors_for_combination(multiplex_net, list(comb)) \n",
    "                                  for comb in two_layer_combinations}\n",
    "print(\"\\nExclusive neighbors for two-layer combinations:\")\n",
    "for comb, neighbors in exclusive_neighbors_two_layers.items():\n",
    "    print(f\"Combination {comb}: {neighbors}\")\n",
    "\n",
    "# Step 3: Exclusive Neighbors for Three-Layer Combinations\n",
    "three_layer_combinations = list(itertools.combinations(all_layers, 3))\n",
    "exclusive_neighbors_three_layers = {comb: find_exclusive_neighbors_for_combination(multiplex_net, list(comb)) \n",
    "                                    for comb in three_layer_combinations}\n",
    "print(\"\\nExclusive neighbors for three-layer combinations:\")\n",
    "for comb, neighbors in exclusive_neighbors_three_layers.items():\n",
    "    print(f\"Combination {comb}: {neighbors}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard scores for individual layers:\n",
      "Layer Layer_1: {'176.0 - 174.0': 1.0, '36.0 - 11.0': 1.0, '36.0 - 2.0': 1.0, '11.0 - 2.0': 1.0}\n",
      "Layer Layer_2: {}\n",
      "Layer Layer_4: {}\n",
      "Layer Layer_5: {'75.0 - 39.0': 0.5, '19.0 - 28.0': 0.3333333333333333, '33.0 - 11.0': 0.5, '39.0 - 20.0': 0.3333333333333333, '39.0 - 62.0': 0.3333333333333333, '20.0 - 62.0': 0.3333333333333333, '11.0 - 28.0': 0.25, '62.0 - 37.0': 0.3333333333333333, '141.0 - 133.0': 1.0, '17.0 - 28.0': 0.25}\n",
      "Layer Layer_3: {}\n",
      "\n",
      "Jaccard scores for two-layer combinations:\n",
      "Combination ('Layer_1', 'Layer_2'): {'176.0 - 174.0': 1.0, '36.0 - 11.0': 1.0, '36.0 - 2.0': 1.0, '11.0 - 2.0': 1.0}\n",
      "Combination ('Layer_1', 'Layer_4'): {'176.0 - 174.0': 1.0, '36.0 - 11.0': 1.0, '36.0 - 2.0': 1.0, '11.0 - 2.0': 1.0}\n",
      "Combination ('Layer_1', 'Layer_5'): {'176.0 - 174.0': 1.0, '75.0 - 39.0': 0.5, '19.0 - 28.0': 0.3333333333333333, '36.0 - 11.0': 0.3333333333333333, '36.0 - 2.0': 1.0, '33.0 - 11.0': 0.3333333333333333, '39.0 - 20.0': 0.3333333333333333, '39.0 - 62.0': 0.3333333333333333, '20.0 - 62.0': 0.3333333333333333, '11.0 - 2.0': 0.3333333333333333, '11.0 - 28.0': 0.2, '6.0 - 62.0': 0.25, '6.0 - 37.0': 0.25, '62.0 - 37.0': 0.3333333333333333, '141.0 - 133.0': 1.0, '17.0 - 28.0': 0.25}\n",
      "Combination ('Layer_1', 'Layer_3'): {'176.0 - 174.0': 1.0, '36.0 - 11.0': 1.0, '36.0 - 2.0': 1.0, '11.0 - 2.0': 1.0}\n",
      "Combination ('Layer_2', 'Layer_4'): {}\n",
      "Combination ('Layer_2', 'Layer_5'): {'75.0 - 39.0': 0.5, '19.0 - 28.0': 0.3333333333333333, '33.0 - 11.0': 0.5, '39.0 - 20.0': 0.3333333333333333, '39.0 - 62.0': 0.3333333333333333, '20.0 - 62.0': 0.3333333333333333, '11.0 - 28.0': 0.25, '62.0 - 37.0': 0.3333333333333333, '141.0 - 133.0': 1.0, '17.0 - 28.0': 0.25}\n",
      "Combination ('Layer_2', 'Layer_3'): {}\n",
      "Combination ('Layer_4', 'Layer_5'): {'75.0 - 39.0': 0.5, '19.0 - 28.0': 0.3333333333333333, '33.0 - 11.0': 0.5, '39.0 - 20.0': 0.3333333333333333, '39.0 - 62.0': 0.3333333333333333, '20.0 - 62.0': 0.3333333333333333, '11.0 - 28.0': 0.25, '62.0 - 37.0': 0.3333333333333333, '141.0 - 133.0': 1.0, '17.0 - 28.0': 0.25}\n",
      "Combination ('Layer_4', 'Layer_3'): {}\n",
      "Combination ('Layer_5', 'Layer_3'): {'75.0 - 39.0': 0.5, '19.0 - 28.0': 0.3333333333333333, '33.0 - 11.0': 0.5, '39.0 - 20.0': 0.3333333333333333, '39.0 - 62.0': 0.3333333333333333, '20.0 - 62.0': 0.3333333333333333, '11.0 - 28.0': 0.25, '62.0 - 37.0': 0.3333333333333333, '141.0 - 133.0': 1.0, '17.0 - 28.0': 0.25}\n",
      "\n",
      "Jaccard scores for three-layer combinations:\n",
      "Combination ('Layer_1', 'Layer_2', 'Layer_4'): {'176.0 - 174.0': 1.0, '36.0 - 11.0': 1.0, '36.0 - 2.0': 1.0, '11.0 - 2.0': 1.0}\n",
      "Combination ('Layer_1', 'Layer_2', 'Layer_5'): {'176.0 - 174.0': 1.0, '75.0 - 39.0': 0.5, '19.0 - 28.0': 0.3333333333333333, '36.0 - 11.0': 0.3333333333333333, '36.0 - 2.0': 1.0, '33.0 - 11.0': 0.3333333333333333, '39.0 - 20.0': 0.3333333333333333, '39.0 - 62.0': 0.3333333333333333, '20.0 - 62.0': 0.3333333333333333, '11.0 - 2.0': 0.3333333333333333, '11.0 - 28.0': 0.2, '6.0 - 62.0': 0.25, '6.0 - 37.0': 0.25, '62.0 - 37.0': 0.3333333333333333, '141.0 - 133.0': 1.0, '17.0 - 28.0': 0.25}\n",
      "Combination ('Layer_1', 'Layer_2', 'Layer_3'): {'176.0 - 174.0': 1.0, '36.0 - 11.0': 1.0, '36.0 - 2.0': 1.0, '11.0 - 2.0': 1.0}\n",
      "Combination ('Layer_1', 'Layer_4', 'Layer_5'): {'176.0 - 174.0': 1.0, '75.0 - 39.0': 0.5, '19.0 - 28.0': 0.3333333333333333, '36.0 - 11.0': 0.3333333333333333, '36.0 - 2.0': 1.0, '33.0 - 11.0': 0.3333333333333333, '39.0 - 20.0': 0.3333333333333333, '39.0 - 62.0': 0.3333333333333333, '20.0 - 62.0': 0.3333333333333333, '11.0 - 2.0': 0.3333333333333333, '11.0 - 28.0': 0.2, '6.0 - 62.0': 0.25, '6.0 - 37.0': 0.25, '62.0 - 37.0': 0.3333333333333333, '141.0 - 133.0': 1.0, '17.0 - 28.0': 0.25}\n",
      "Combination ('Layer_1', 'Layer_4', 'Layer_3'): {'176.0 - 174.0': 1.0, '36.0 - 11.0': 1.0, '36.0 - 2.0': 1.0, '11.0 - 2.0': 1.0}\n",
      "Combination ('Layer_1', 'Layer_5', 'Layer_3'): {'176.0 - 174.0': 1.0, '75.0 - 39.0': 0.5, '19.0 - 28.0': 0.3333333333333333, '36.0 - 11.0': 0.3333333333333333, '36.0 - 2.0': 1.0, '33.0 - 11.0': 0.3333333333333333, '39.0 - 20.0': 0.3333333333333333, '39.0 - 62.0': 0.3333333333333333, '20.0 - 62.0': 0.3333333333333333, '11.0 - 2.0': 0.3333333333333333, '11.0 - 28.0': 0.2, '6.0 - 62.0': 0.25, '6.0 - 37.0': 0.25, '62.0 - 37.0': 0.3333333333333333, '141.0 - 133.0': 1.0, '17.0 - 28.0': 0.25}\n",
      "Combination ('Layer_2', 'Layer_4', 'Layer_5'): {'75.0 - 39.0': 0.5, '19.0 - 28.0': 0.3333333333333333, '33.0 - 11.0': 0.5, '39.0 - 20.0': 0.3333333333333333, '39.0 - 62.0': 0.3333333333333333, '20.0 - 62.0': 0.3333333333333333, '11.0 - 28.0': 0.25, '62.0 - 37.0': 0.3333333333333333, '141.0 - 133.0': 1.0, '17.0 - 28.0': 0.25}\n",
      "Combination ('Layer_2', 'Layer_4', 'Layer_3'): {}\n",
      "Combination ('Layer_2', 'Layer_5', 'Layer_3'): {'75.0 - 39.0': 0.5, '19.0 - 28.0': 0.3333333333333333, '33.0 - 11.0': 0.5, '39.0 - 20.0': 0.3333333333333333, '39.0 - 62.0': 0.3333333333333333, '20.0 - 62.0': 0.3333333333333333, '11.0 - 28.0': 0.25, '62.0 - 37.0': 0.3333333333333333, '141.0 - 133.0': 1.0, '17.0 - 28.0': 0.25}\n",
      "Combination ('Layer_4', 'Layer_5', 'Layer_3'): {'75.0 - 39.0': 0.5, '19.0 - 28.0': 0.3333333333333333, '33.0 - 11.0': 0.5, '39.0 - 20.0': 0.3333333333333333, '39.0 - 62.0': 0.3333333333333333, '20.0 - 62.0': 0.3333333333333333, '11.0 - 28.0': 0.25, '62.0 - 37.0': 0.3333333333333333, '141.0 - 133.0': 1.0, '17.0 - 28.0': 0.25}\n"
     ]
    }
   ],
   "source": [
    "def modified_jaccard_coefficient(G, exclusive_neighbors):\n",
    "    def predict(u, v):\n",
    "        neighbors_u = exclusive_neighbors.get(u, set())\n",
    "        neighbors_v = exclusive_neighbors.get(v, set())\n",
    "        union_size = len(neighbors_u | neighbors_v)\n",
    "        if union_size == 0:\n",
    "            return 0\n",
    "        intersection_size = len(neighbors_u & neighbors_v)\n",
    "        return intersection_size / union_size\n",
    "\n",
    "    return ((u, v, predict(u, v)) for u, v in nx.non_edges(G))\n",
    "\n",
    "def calculate_jaccard_scores_for_layers(nx_graphs, layers, exclusive_neighbors):\n",
    "    combined_graph = nx.Graph()\n",
    "    for layer in layers:\n",
    "        combined_graph = nx.compose(combined_graph, nx_graphs[layer])\n",
    "\n",
    "    jaccard_scores = {}\n",
    "    for u, v, score in modified_jaccard_coefficient(combined_graph, exclusive_neighbors):\n",
    "        if score > 0:\n",
    "            jaccard_scores[f\"{u} - {v}\"] = score\n",
    "    return jaccard_scores\n",
    "\n",
    "# Convert the multiplex network into NetworkX graphs for each layer\n",
    "nx_graphs = ml.to_nx_dict(multiplex_net)\n",
    "\n",
    "# Calculate Jaccard scores for individual layers\n",
    "jaccard_scores_individual_layers = {layer: calculate_jaccard_scores_for_layers(nx_graphs, [layer], exclusive_neighbors_individual_layers[layer])\n",
    "                                    for layer in all_layers}\n",
    "print(\"Jaccard scores for individual layers:\")\n",
    "for layer, scores in jaccard_scores_individual_layers.items():\n",
    "    print(f\"Layer {layer}: {scores}\")\n",
    "\n",
    "# Calculate Jaccard scores for two-layer combinations\n",
    "jaccard_scores_two_layers = {comb: calculate_jaccard_scores_for_layers(nx_graphs, list(comb), exclusive_neighbors_two_layers[comb])\n",
    "                             for comb in two_layer_combinations}\n",
    "print(\"\\nJaccard scores for two-layer combinations:\")\n",
    "for comb, scores in jaccard_scores_two_layers.items():\n",
    "    print(f\"Combination {comb}: {scores}\")\n",
    "\n",
    "# Calculate Jaccard scores for three-layer combinations\n",
    "jaccard_scores_three_layers = {comb: calculate_jaccard_scores_for_layers(nx_graphs, list(comb), exclusive_neighbors_three_layers[comb])\n",
    "                               for comb in three_layer_combinations}\n",
    "print(\"\\nJaccard scores for three-layer combinations:\")\n",
    "for comb, scores in jaccard_scores_three_layers.items():\n",
    "    print(f\"Combination {comb}: {scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adamic-Adar scores for individual layers:\n",
      "Layer Layer_1: {'176.0 - 174.0': 1.4426950408889634, '36.0 - 11.0': 0.9102392266268373, '36.0 - 2.0': 0.9102392266268373, '11.0 - 2.0': 0.9102392266268373}\n",
      "Layer Layer_2: {}\n",
      "Layer Layer_4: {}\n",
      "Layer Layer_5: {'75.0 - 39.0': 1.4426950408889634, '19.0 - 28.0': 1.4426950408889634, '33.0 - 11.0': 1.4426950408889634, '39.0 - 20.0': 0.9102392266268373, '39.0 - 62.0': 0.9102392266268373, '20.0 - 62.0': 0.9102392266268373, '11.0 - 28.0': 1.4426950408889634, '62.0 - 37.0': 1.4426950408889634, '141.0 - 133.0': 1.4426950408889634, '17.0 - 28.0': 1.4426950408889634}\n",
      "Layer Layer_3: {}\n",
      "\n",
      "Adamic-Adar scores for two-layer combinations:\n",
      "Combination ('Layer_1', 'Layer_2'): {'176.0 - 174.0': 1.4426950408889634, '36.0 - 11.0': 0.9102392266268373, '36.0 - 2.0': 0.9102392266268373, '11.0 - 2.0': 0.9102392266268373}\n",
      "Combination ('Layer_1', 'Layer_4'): {'176.0 - 174.0': 1.4426950408889634, '36.0 - 11.0': 0.9102392266268373, '36.0 - 2.0': 0.9102392266268373, '11.0 - 2.0': 0.9102392266268373}\n",
      "Combination ('Layer_1', 'Layer_5'): {'176.0 - 174.0': 1.4426950408889634, '75.0 - 39.0': 1.4426950408889634, '19.0 - 28.0': 1.4426950408889634, '36.0 - 11.0': 0.9102392266268373, '36.0 - 2.0': 0.9102392266268373, '33.0 - 11.0': 1.4426950408889634, '39.0 - 20.0': 0.9102392266268373, '39.0 - 62.0': 0.9102392266268373, '20.0 - 62.0': 0.9102392266268373, '11.0 - 2.0': 0.9102392266268373, '11.0 - 28.0': 1.4426950408889634, '6.0 - 62.0': 0.9102392266268373, '6.0 - 37.0': 0.9102392266268373, '62.0 - 37.0': 0.9102392266268373, '141.0 - 133.0': 1.4426950408889634, '17.0 - 28.0': 1.4426950408889634}\n",
      "Combination ('Layer_1', 'Layer_3'): {'176.0 - 174.0': 1.4426950408889634, '36.0 - 11.0': 0.9102392266268373, '36.0 - 2.0': 0.9102392266268373, '11.0 - 2.0': 0.9102392266268373}\n",
      "Combination ('Layer_2', 'Layer_4'): {}\n",
      "Combination ('Layer_2', 'Layer_5'): {'75.0 - 39.0': 1.4426950408889634, '19.0 - 28.0': 1.4426950408889634, '33.0 - 11.0': 1.4426950408889634, '39.0 - 20.0': 0.9102392266268373, '39.0 - 62.0': 0.9102392266268373, '20.0 - 62.0': 0.9102392266268373, '11.0 - 28.0': 1.4426950408889634, '62.0 - 37.0': 1.4426950408889634, '141.0 - 133.0': 1.4426950408889634, '17.0 - 28.0': 1.4426950408889634}\n",
      "Combination ('Layer_2', 'Layer_3'): {}\n",
      "Combination ('Layer_4', 'Layer_5'): {'75.0 - 39.0': 1.4426950408889634, '19.0 - 28.0': 1.4426950408889634, '33.0 - 11.0': 1.4426950408889634, '39.0 - 20.0': 0.9102392266268373, '39.0 - 62.0': 0.9102392266268373, '20.0 - 62.0': 0.9102392266268373, '11.0 - 28.0': 1.4426950408889634, '62.0 - 37.0': 1.4426950408889634, '141.0 - 133.0': 1.4426950408889634, '17.0 - 28.0': 1.4426950408889634}\n",
      "Combination ('Layer_4', 'Layer_3'): {}\n",
      "Combination ('Layer_5', 'Layer_3'): {'75.0 - 39.0': 1.4426950408889634, '19.0 - 28.0': 1.4426950408889634, '33.0 - 11.0': 1.4426950408889634, '39.0 - 20.0': 0.9102392266268373, '39.0 - 62.0': 0.9102392266268373, '20.0 - 62.0': 0.9102392266268373, '11.0 - 28.0': 1.4426950408889634, '62.0 - 37.0': 1.4426950408889634, '141.0 - 133.0': 1.4426950408889634, '17.0 - 28.0': 1.4426950408889634}\n",
      "\n",
      "Adamic-Adar scores for three-layer combinations:\n",
      "Combination ('Layer_1', 'Layer_2', 'Layer_4'): {'176.0 - 174.0': 1.4426950408889634, '36.0 - 11.0': 0.9102392266268373, '36.0 - 2.0': 0.9102392266268373, '11.0 - 2.0': 0.9102392266268373}\n",
      "Combination ('Layer_1', 'Layer_2', 'Layer_5'): {'176.0 - 174.0': 1.4426950408889634, '75.0 - 39.0': 1.4426950408889634, '19.0 - 28.0': 1.4426950408889634, '36.0 - 11.0': 0.9102392266268373, '36.0 - 2.0': 0.9102392266268373, '33.0 - 11.0': 1.4426950408889634, '39.0 - 20.0': 0.9102392266268373, '39.0 - 62.0': 0.9102392266268373, '20.0 - 62.0': 0.9102392266268373, '11.0 - 2.0': 0.9102392266268373, '11.0 - 28.0': 1.4426950408889634, '6.0 - 62.0': 0.9102392266268373, '6.0 - 37.0': 0.9102392266268373, '62.0 - 37.0': 0.9102392266268373, '141.0 - 133.0': 1.4426950408889634, '17.0 - 28.0': 1.4426950408889634}\n",
      "Combination ('Layer_1', 'Layer_2', 'Layer_3'): {'176.0 - 174.0': 1.4426950408889634, '36.0 - 11.0': 0.9102392266268373, '36.0 - 2.0': 0.9102392266268373, '11.0 - 2.0': 0.9102392266268373}\n",
      "Combination ('Layer_1', 'Layer_4', 'Layer_5'): {'176.0 - 174.0': 1.4426950408889634, '75.0 - 39.0': 1.4426950408889634, '19.0 - 28.0': 1.4426950408889634, '36.0 - 11.0': 0.9102392266268373, '36.0 - 2.0': 0.9102392266268373, '33.0 - 11.0': 1.4426950408889634, '39.0 - 20.0': 0.9102392266268373, '39.0 - 62.0': 0.9102392266268373, '20.0 - 62.0': 0.9102392266268373, '11.0 - 2.0': 0.9102392266268373, '11.0 - 28.0': 1.4426950408889634, '6.0 - 62.0': 0.9102392266268373, '6.0 - 37.0': 0.9102392266268373, '62.0 - 37.0': 0.9102392266268373, '141.0 - 133.0': 1.4426950408889634, '17.0 - 28.0': 1.4426950408889634}\n",
      "Combination ('Layer_1', 'Layer_4', 'Layer_3'): {'176.0 - 174.0': 1.4426950408889634, '36.0 - 11.0': 0.9102392266268373, '36.0 - 2.0': 0.9102392266268373, '11.0 - 2.0': 0.9102392266268373}\n",
      "Combination ('Layer_1', 'Layer_5', 'Layer_3'): {'176.0 - 174.0': 1.4426950408889634, '75.0 - 39.0': 1.4426950408889634, '19.0 - 28.0': 1.4426950408889634, '36.0 - 11.0': 0.9102392266268373, '36.0 - 2.0': 0.9102392266268373, '33.0 - 11.0': 1.4426950408889634, '39.0 - 20.0': 0.9102392266268373, '39.0 - 62.0': 0.9102392266268373, '20.0 - 62.0': 0.9102392266268373, '11.0 - 2.0': 0.9102392266268373, '11.0 - 28.0': 1.4426950408889634, '6.0 - 62.0': 0.9102392266268373, '6.0 - 37.0': 0.9102392266268373, '62.0 - 37.0': 0.9102392266268373, '141.0 - 133.0': 1.4426950408889634, '17.0 - 28.0': 1.4426950408889634}\n",
      "Combination ('Layer_2', 'Layer_4', 'Layer_5'): {'75.0 - 39.0': 1.4426950408889634, '19.0 - 28.0': 1.4426950408889634, '33.0 - 11.0': 1.4426950408889634, '39.0 - 20.0': 0.9102392266268373, '39.0 - 62.0': 0.9102392266268373, '20.0 - 62.0': 0.9102392266268373, '11.0 - 28.0': 1.4426950408889634, '62.0 - 37.0': 1.4426950408889634, '141.0 - 133.0': 1.4426950408889634, '17.0 - 28.0': 1.4426950408889634}\n",
      "Combination ('Layer_2', 'Layer_4', 'Layer_3'): {}\n",
      "Combination ('Layer_2', 'Layer_5', 'Layer_3'): {'75.0 - 39.0': 1.4426950408889634, '19.0 - 28.0': 1.4426950408889634, '33.0 - 11.0': 1.4426950408889634, '39.0 - 20.0': 0.9102392266268373, '39.0 - 62.0': 0.9102392266268373, '20.0 - 62.0': 0.9102392266268373, '11.0 - 28.0': 1.4426950408889634, '62.0 - 37.0': 1.4426950408889634, '141.0 - 133.0': 1.4426950408889634, '17.0 - 28.0': 1.4426950408889634}\n",
      "Combination ('Layer_4', 'Layer_5', 'Layer_3'): {'75.0 - 39.0': 1.4426950408889634, '19.0 - 28.0': 1.4426950408889634, '33.0 - 11.0': 1.4426950408889634, '39.0 - 20.0': 0.9102392266268373, '39.0 - 62.0': 0.9102392266268373, '20.0 - 62.0': 0.9102392266268373, '11.0 - 28.0': 1.4426950408889634, '62.0 - 37.0': 1.4426950408889634, '141.0 - 133.0': 1.4426950408889634, '17.0 - 28.0': 1.4426950408889634}\n"
     ]
    }
   ],
   "source": [
    "def modified_adamic_adar_index(G, exclusive_neighbors):\n",
    "    def predict(u, v):\n",
    "        common_neighbors = exclusive_neighbors.get(u, set()) & exclusive_neighbors.get(v, set())\n",
    "        return sum(1 / log(len(exclusive_neighbors.get(w, set()))) for w in common_neighbors if len(exclusive_neighbors.get(w, set())) > 1)\n",
    "\n",
    "    return ((u, v, predict(u, v)) for u, v in nx.non_edges(G))\n",
    "\n",
    "def calculate_adamic_adar_scores_for_layers(nx_graphs, layers, exclusive_neighbors):\n",
    "    combined_graph = nx.Graph()\n",
    "    for layer in layers:\n",
    "        combined_graph = nx.compose(combined_graph, nx_graphs[layer])\n",
    "\n",
    "    adamic_adar_scores = {}\n",
    "    for u, v, score in modified_adamic_adar_index(combined_graph, exclusive_neighbors):\n",
    "        if score > 0:\n",
    "            adamic_adar_scores[f\"{u} - {v}\"] = score\n",
    "    return adamic_adar_scores\n",
    "\n",
    "# Convert the multiplex network into NetworkX graphs for each layer\n",
    "nx_graphs = ml.to_nx_dict(multiplex_net)\n",
    "\n",
    "# Calculate Adamic-Adar scores for individual layers\n",
    "adamic_adar_scores_individual_layers = {layer: calculate_adamic_adar_scores_for_layers(nx_graphs, [layer], exclusive_neighbors_individual_layers[layer])\n",
    "                                        for layer in all_layers}\n",
    "print(\"Adamic-Adar scores for individual layers:\")\n",
    "for layer, scores in adamic_adar_scores_individual_layers.items():\n",
    "    print(f\"Layer {layer}: {scores}\")\n",
    "\n",
    "# Calculate Adamic-Adar scores for two-layer combinations\n",
    "adamic_adar_scores_two_layers = {comb: calculate_adamic_adar_scores_for_layers(nx_graphs, list(comb), exclusive_neighbors_two_layers[comb])\n",
    "                                 for comb in two_layer_combinations}\n",
    "print(\"\\nAdamic-Adar scores for two-layer combinations:\")\n",
    "for comb, scores in adamic_adar_scores_two_layers.items():\n",
    "    print(f\"Combination {comb}: {scores}\")\n",
    "\n",
    "# Calculate Adamic-Adar scores for three-layer combinations\n",
    "adamic_adar_scores_three_layers = {comb: calculate_adamic_adar_scores_for_layers(nx_graphs, list(comb), exclusive_neighbors_three_layers[comb])\n",
    "                                   for comb in three_layer_combinations}\n",
    "print(\"\\nAdamic-Adar scores for three-layer combinations:\")\n",
    "for comb, scores in adamic_adar_scores_three_layers.items():\n",
    "    print(f\"Combination {comb}: {scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Jaccard Scores for Individual Layers:\n",
      "Layer_1: {'176.0 - 174.0': 1.0, '36.0 - 11.0': 1.0, '36.0 - 2.0': 1.0, '11.0 - 2.0': 1.0}\n",
      "Layer_5: {'75.0 - 39.0': 0.5, '19.0 - 28.0': 0.3333333333333333, '33.0 - 11.0': 0.5, '39.0 - 20.0': 0.3333333333333333, '39.0 - 62.0': 0.3333333333333333, '20.0 - 62.0': 0.3333333333333333, '11.0 - 28.0': 0.25, '62.0 - 37.0': 0.3333333333333333, '141.0 - 133.0': 1.0, '17.0 - 28.0': 0.25}\n",
      "\n",
      "Normalized Jaccard Scores for Two-Layer Combinations:\n",
      "('Layer_1', 'Layer_2'): {'176.0 - 174.0': 1.0, '36.0 - 11.0': 1.0, '36.0 - 2.0': 1.0, '11.0 - 2.0': 1.0}\n",
      "('Layer_1', 'Layer_4'): {'176.0 - 174.0': 1.0, '36.0 - 11.0': 1.0, '36.0 - 2.0': 1.0, '11.0 - 2.0': 1.0}\n",
      "('Layer_1', 'Layer_5'): {'176.0 - 174.0': 1.0, '75.0 - 39.0': 0.5, '19.0 - 28.0': 0.3333333333333333, '36.0 - 11.0': 0.3333333333333333, '36.0 - 2.0': 1.0, '33.0 - 11.0': 0.3333333333333333, '39.0 - 20.0': 0.3333333333333333, '39.0 - 62.0': 0.3333333333333333, '20.0 - 62.0': 0.3333333333333333, '11.0 - 2.0': 0.3333333333333333, '11.0 - 28.0': 0.2, '6.0 - 62.0': 0.25, '6.0 - 37.0': 0.25, '62.0 - 37.0': 0.3333333333333333, '141.0 - 133.0': 1.0, '17.0 - 28.0': 0.25}\n",
      "('Layer_1', 'Layer_3'): {'176.0 - 174.0': 1.0, '36.0 - 11.0': 1.0, '36.0 - 2.0': 1.0, '11.0 - 2.0': 1.0}\n",
      "('Layer_2', 'Layer_5'): {'75.0 - 39.0': 0.5, '19.0 - 28.0': 0.3333333333333333, '33.0 - 11.0': 0.5, '39.0 - 20.0': 0.3333333333333333, '39.0 - 62.0': 0.3333333333333333, '20.0 - 62.0': 0.3333333333333333, '11.0 - 28.0': 0.25, '62.0 - 37.0': 0.3333333333333333, '141.0 - 133.0': 1.0, '17.0 - 28.0': 0.25}\n",
      "('Layer_4', 'Layer_5'): {'75.0 - 39.0': 0.5, '19.0 - 28.0': 0.3333333333333333, '33.0 - 11.0': 0.5, '39.0 - 20.0': 0.3333333333333333, '39.0 - 62.0': 0.3333333333333333, '20.0 - 62.0': 0.3333333333333333, '11.0 - 28.0': 0.25, '62.0 - 37.0': 0.3333333333333333, '141.0 - 133.0': 1.0, '17.0 - 28.0': 0.25}\n",
      "('Layer_5', 'Layer_3'): {'75.0 - 39.0': 0.5, '19.0 - 28.0': 0.3333333333333333, '33.0 - 11.0': 0.5, '39.0 - 20.0': 0.3333333333333333, '39.0 - 62.0': 0.3333333333333333, '20.0 - 62.0': 0.3333333333333333, '11.0 - 28.0': 0.25, '62.0 - 37.0': 0.3333333333333333, '141.0 - 133.0': 1.0, '17.0 - 28.0': 0.25}\n",
      "\n",
      "Normalized Jaccard Scores for Three-Layer Combinations:\n",
      "('Layer_1', 'Layer_2', 'Layer_4'): {'176.0 - 174.0': 1.0, '36.0 - 11.0': 1.0, '36.0 - 2.0': 1.0, '11.0 - 2.0': 1.0}\n",
      "('Layer_1', 'Layer_2', 'Layer_5'): {'176.0 - 174.0': 1.0, '75.0 - 39.0': 0.5, '19.0 - 28.0': 0.3333333333333333, '36.0 - 11.0': 0.3333333333333333, '36.0 - 2.0': 1.0, '33.0 - 11.0': 0.3333333333333333, '39.0 - 20.0': 0.3333333333333333, '39.0 - 62.0': 0.3333333333333333, '20.0 - 62.0': 0.3333333333333333, '11.0 - 2.0': 0.3333333333333333, '11.0 - 28.0': 0.2, '6.0 - 62.0': 0.25, '6.0 - 37.0': 0.25, '62.0 - 37.0': 0.3333333333333333, '141.0 - 133.0': 1.0, '17.0 - 28.0': 0.25}\n",
      "('Layer_1', 'Layer_2', 'Layer_3'): {'176.0 - 174.0': 1.0, '36.0 - 11.0': 1.0, '36.0 - 2.0': 1.0, '11.0 - 2.0': 1.0}\n",
      "('Layer_1', 'Layer_4', 'Layer_5'): {'176.0 - 174.0': 1.0, '75.0 - 39.0': 0.5, '19.0 - 28.0': 0.3333333333333333, '36.0 - 11.0': 0.3333333333333333, '36.0 - 2.0': 1.0, '33.0 - 11.0': 0.3333333333333333, '39.0 - 20.0': 0.3333333333333333, '39.0 - 62.0': 0.3333333333333333, '20.0 - 62.0': 0.3333333333333333, '11.0 - 2.0': 0.3333333333333333, '11.0 - 28.0': 0.2, '6.0 - 62.0': 0.25, '6.0 - 37.0': 0.25, '62.0 - 37.0': 0.3333333333333333, '141.0 - 133.0': 1.0, '17.0 - 28.0': 0.25}\n",
      "('Layer_1', 'Layer_4', 'Layer_3'): {'176.0 - 174.0': 1.0, '36.0 - 11.0': 1.0, '36.0 - 2.0': 1.0, '11.0 - 2.0': 1.0}\n",
      "('Layer_1', 'Layer_5', 'Layer_3'): {'176.0 - 174.0': 1.0, '75.0 - 39.0': 0.5, '19.0 - 28.0': 0.3333333333333333, '36.0 - 11.0': 0.3333333333333333, '36.0 - 2.0': 1.0, '33.0 - 11.0': 0.3333333333333333, '39.0 - 20.0': 0.3333333333333333, '39.0 - 62.0': 0.3333333333333333, '20.0 - 62.0': 0.3333333333333333, '11.0 - 2.0': 0.3333333333333333, '11.0 - 28.0': 0.2, '6.0 - 62.0': 0.25, '6.0 - 37.0': 0.25, '62.0 - 37.0': 0.3333333333333333, '141.0 - 133.0': 1.0, '17.0 - 28.0': 0.25}\n",
      "('Layer_2', 'Layer_4', 'Layer_5'): {'75.0 - 39.0': 0.5, '19.0 - 28.0': 0.3333333333333333, '33.0 - 11.0': 0.5, '39.0 - 20.0': 0.3333333333333333, '39.0 - 62.0': 0.3333333333333333, '20.0 - 62.0': 0.3333333333333333, '11.0 - 28.0': 0.25, '62.0 - 37.0': 0.3333333333333333, '141.0 - 133.0': 1.0, '17.0 - 28.0': 0.25}\n",
      "('Layer_2', 'Layer_5', 'Layer_3'): {'75.0 - 39.0': 0.5, '19.0 - 28.0': 0.3333333333333333, '33.0 - 11.0': 0.5, '39.0 - 20.0': 0.3333333333333333, '39.0 - 62.0': 0.3333333333333333, '20.0 - 62.0': 0.3333333333333333, '11.0 - 28.0': 0.25, '62.0 - 37.0': 0.3333333333333333, '141.0 - 133.0': 1.0, '17.0 - 28.0': 0.25}\n",
      "('Layer_4', 'Layer_5', 'Layer_3'): {'75.0 - 39.0': 0.5, '19.0 - 28.0': 0.3333333333333333, '33.0 - 11.0': 0.5, '39.0 - 20.0': 0.3333333333333333, '39.0 - 62.0': 0.3333333333333333, '20.0 - 62.0': 0.3333333333333333, '11.0 - 28.0': 0.25, '62.0 - 37.0': 0.3333333333333333, '141.0 - 133.0': 1.0, '17.0 - 28.0': 0.25}\n",
      "\n",
      "Normalized Adamic-Adar Scores for Individual Layers:\n",
      "Layer_1: {'176.0 - 174.0': 1.0, '36.0 - 11.0': 0.6309297535714574, '36.0 - 2.0': 0.6309297535714574, '11.0 - 2.0': 0.6309297535714574}\n",
      "Layer_5: {'75.0 - 39.0': 1.0, '19.0 - 28.0': 1.0, '33.0 - 11.0': 1.0, '39.0 - 20.0': 0.6309297535714574, '39.0 - 62.0': 0.6309297535714574, '20.0 - 62.0': 0.6309297535714574, '11.0 - 28.0': 1.0, '62.0 - 37.0': 1.0, '141.0 - 133.0': 1.0, '17.0 - 28.0': 1.0}\n",
      "\n",
      "Normalized Adamic-Adar Scores for Two-Layer Combinations:\n",
      "('Layer_1', 'Layer_2'): {'176.0 - 174.0': 1.0, '36.0 - 11.0': 0.6309297535714574, '36.0 - 2.0': 0.6309297535714574, '11.0 - 2.0': 0.6309297535714574}\n",
      "('Layer_1', 'Layer_4'): {'176.0 - 174.0': 1.0, '36.0 - 11.0': 0.6309297535714574, '36.0 - 2.0': 0.6309297535714574, '11.0 - 2.0': 0.6309297535714574}\n",
      "('Layer_1', 'Layer_5'): {'176.0 - 174.0': 1.0, '75.0 - 39.0': 1.0, '19.0 - 28.0': 1.0, '36.0 - 11.0': 0.6309297535714574, '36.0 - 2.0': 0.6309297535714574, '33.0 - 11.0': 1.0, '39.0 - 20.0': 0.6309297535714574, '39.0 - 62.0': 0.6309297535714574, '20.0 - 62.0': 0.6309297535714574, '11.0 - 2.0': 0.6309297535714574, '11.0 - 28.0': 1.0, '6.0 - 62.0': 0.6309297535714574, '6.0 - 37.0': 0.6309297535714574, '62.0 - 37.0': 0.6309297535714574, '141.0 - 133.0': 1.0, '17.0 - 28.0': 1.0}\n",
      "('Layer_1', 'Layer_3'): {'176.0 - 174.0': 1.0, '36.0 - 11.0': 0.6309297535714574, '36.0 - 2.0': 0.6309297535714574, '11.0 - 2.0': 0.6309297535714574}\n",
      "('Layer_2', 'Layer_5'): {'75.0 - 39.0': 1.0, '19.0 - 28.0': 1.0, '33.0 - 11.0': 1.0, '39.0 - 20.0': 0.6309297535714574, '39.0 - 62.0': 0.6309297535714574, '20.0 - 62.0': 0.6309297535714574, '11.0 - 28.0': 1.0, '62.0 - 37.0': 1.0, '141.0 - 133.0': 1.0, '17.0 - 28.0': 1.0}\n",
      "('Layer_4', 'Layer_5'): {'75.0 - 39.0': 1.0, '19.0 - 28.0': 1.0, '33.0 - 11.0': 1.0, '39.0 - 20.0': 0.6309297535714574, '39.0 - 62.0': 0.6309297535714574, '20.0 - 62.0': 0.6309297535714574, '11.0 - 28.0': 1.0, '62.0 - 37.0': 1.0, '141.0 - 133.0': 1.0, '17.0 - 28.0': 1.0}\n",
      "('Layer_5', 'Layer_3'): {'75.0 - 39.0': 1.0, '19.0 - 28.0': 1.0, '33.0 - 11.0': 1.0, '39.0 - 20.0': 0.6309297535714574, '39.0 - 62.0': 0.6309297535714574, '20.0 - 62.0': 0.6309297535714574, '11.0 - 28.0': 1.0, '62.0 - 37.0': 1.0, '141.0 - 133.0': 1.0, '17.0 - 28.0': 1.0}\n",
      "\n",
      "Normalized Adamic-Adar Scores for Three-Layer Combinations:\n",
      "('Layer_1', 'Layer_2', 'Layer_4'): {'176.0 - 174.0': 1.0, '36.0 - 11.0': 0.6309297535714574, '36.0 - 2.0': 0.6309297535714574, '11.0 - 2.0': 0.6309297535714574}\n",
      "('Layer_1', 'Layer_2', 'Layer_5'): {'176.0 - 174.0': 1.0, '75.0 - 39.0': 1.0, '19.0 - 28.0': 1.0, '36.0 - 11.0': 0.6309297535714574, '36.0 - 2.0': 0.6309297535714574, '33.0 - 11.0': 1.0, '39.0 - 20.0': 0.6309297535714574, '39.0 - 62.0': 0.6309297535714574, '20.0 - 62.0': 0.6309297535714574, '11.0 - 2.0': 0.6309297535714574, '11.0 - 28.0': 1.0, '6.0 - 62.0': 0.6309297535714574, '6.0 - 37.0': 0.6309297535714574, '62.0 - 37.0': 0.6309297535714574, '141.0 - 133.0': 1.0, '17.0 - 28.0': 1.0}\n",
      "('Layer_1', 'Layer_2', 'Layer_3'): {'176.0 - 174.0': 1.0, '36.0 - 11.0': 0.6309297535714574, '36.0 - 2.0': 0.6309297535714574, '11.0 - 2.0': 0.6309297535714574}\n",
      "('Layer_1', 'Layer_4', 'Layer_5'): {'176.0 - 174.0': 1.0, '75.0 - 39.0': 1.0, '19.0 - 28.0': 1.0, '36.0 - 11.0': 0.6309297535714574, '36.0 - 2.0': 0.6309297535714574, '33.0 - 11.0': 1.0, '39.0 - 20.0': 0.6309297535714574, '39.0 - 62.0': 0.6309297535714574, '20.0 - 62.0': 0.6309297535714574, '11.0 - 2.0': 0.6309297535714574, '11.0 - 28.0': 1.0, '6.0 - 62.0': 0.6309297535714574, '6.0 - 37.0': 0.6309297535714574, '62.0 - 37.0': 0.6309297535714574, '141.0 - 133.0': 1.0, '17.0 - 28.0': 1.0}\n",
      "('Layer_1', 'Layer_4', 'Layer_3'): {'176.0 - 174.0': 1.0, '36.0 - 11.0': 0.6309297535714574, '36.0 - 2.0': 0.6309297535714574, '11.0 - 2.0': 0.6309297535714574}\n",
      "('Layer_1', 'Layer_5', 'Layer_3'): {'176.0 - 174.0': 1.0, '75.0 - 39.0': 1.0, '19.0 - 28.0': 1.0, '36.0 - 11.0': 0.6309297535714574, '36.0 - 2.0': 0.6309297535714574, '33.0 - 11.0': 1.0, '39.0 - 20.0': 0.6309297535714574, '39.0 - 62.0': 0.6309297535714574, '20.0 - 62.0': 0.6309297535714574, '11.0 - 2.0': 0.6309297535714574, '11.0 - 28.0': 1.0, '6.0 - 62.0': 0.6309297535714574, '6.0 - 37.0': 0.6309297535714574, '62.0 - 37.0': 0.6309297535714574, '141.0 - 133.0': 1.0, '17.0 - 28.0': 1.0}\n",
      "('Layer_2', 'Layer_4', 'Layer_5'): {'75.0 - 39.0': 1.0, '19.0 - 28.0': 1.0, '33.0 - 11.0': 1.0, '39.0 - 20.0': 0.6309297535714574, '39.0 - 62.0': 0.6309297535714574, '20.0 - 62.0': 0.6309297535714574, '11.0 - 28.0': 1.0, '62.0 - 37.0': 1.0, '141.0 - 133.0': 1.0, '17.0 - 28.0': 1.0}\n",
      "('Layer_2', 'Layer_5', 'Layer_3'): {'75.0 - 39.0': 1.0, '19.0 - 28.0': 1.0, '33.0 - 11.0': 1.0, '39.0 - 20.0': 0.6309297535714574, '39.0 - 62.0': 0.6309297535714574, '20.0 - 62.0': 0.6309297535714574, '11.0 - 28.0': 1.0, '62.0 - 37.0': 1.0, '141.0 - 133.0': 1.0, '17.0 - 28.0': 1.0}\n",
      "('Layer_4', 'Layer_5', 'Layer_3'): {'75.0 - 39.0': 1.0, '19.0 - 28.0': 1.0, '33.0 - 11.0': 1.0, '39.0 - 20.0': 0.6309297535714574, '39.0 - 62.0': 0.6309297535714574, '20.0 - 62.0': 0.6309297535714574, '11.0 - 28.0': 1.0, '62.0 - 37.0': 1.0, '141.0 - 133.0': 1.0, '17.0 - 28.0': 1.0}\n"
     ]
    }
   ],
   "source": [
    "def normalize_by_max(scores):\n",
    "    # Flatten all scores into a single list, excluding empty layers or combinations\n",
    "    all_scores = [score for layer_scores in scores.values() for score in layer_scores.values() if layer_scores]\n",
    "\n",
    "    # Check if there are no scores to normalize\n",
    "    if not all_scores:\n",
    "        print(\"No scores to normalize.\")\n",
    "        return {}\n",
    "\n",
    "    # Find the maximum score for normalization\n",
    "    max_score = max(all_scores)\n",
    "\n",
    "    # Normalize the scores\n",
    "    normalized_scores = {}\n",
    "    for key, layer_scores in scores.items():\n",
    "        if layer_scores:  # Check if the layer or combination has scores\n",
    "            normalized_scores[key] = {link: score / max_score for link, score in layer_scores.items()}\n",
    "    \n",
    "    return normalized_scores\n",
    "\n",
    "# Normalize Jaccard and Adamic-Adar scores for individual layers, two-layer combinations, and three-layer combinations\n",
    "normalized_jaccard_scores_individual_layers = normalize_by_max(jaccard_scores_individual_layers)\n",
    "normalized_jaccard_scores_two_layers = normalize_by_max(jaccard_scores_two_layers)\n",
    "normalized_jaccard_scores_three_layers = normalize_by_max(jaccard_scores_three_layers)\n",
    "\n",
    "normalized_adamic_adar_scores_individual_layers = normalize_by_max(adamic_adar_scores_individual_layers)\n",
    "normalized_adamic_adar_scores_two_layers = normalize_by_max(adamic_adar_scores_two_layers)\n",
    "normalized_adamic_adar_scores_three_layers = normalize_by_max(adamic_adar_scores_three_layers)\n",
    "\n",
    "# Display the normalized scores\n",
    "print(\"Normalized Jaccard Scores for Individual Layers:\")\n",
    "for layer, scores in normalized_jaccard_scores_individual_layers.items():\n",
    "    print(f\"{layer}: {scores}\")\n",
    "\n",
    "print(\"\\nNormalized Jaccard Scores for Two-Layer Combinations:\")\n",
    "for combination, scores in normalized_jaccard_scores_two_layers.items():\n",
    "    print(f\"{combination}: {scores}\")\n",
    "\n",
    "print(\"\\nNormalized Jaccard Scores for Three-Layer Combinations:\")\n",
    "for combination, scores in normalized_jaccard_scores_three_layers.items():\n",
    "    print(f\"{combination}: {scores}\")\n",
    "\n",
    "print(\"\\nNormalized Adamic-Adar Scores for Individual Layers:\")\n",
    "for layer, scores in normalized_adamic_adar_scores_individual_layers.items():\n",
    "    print(f\"{layer}: {scores}\")\n",
    "\n",
    "print(\"\\nNormalized Adamic-Adar Scores for Two-Layer Combinations:\")\n",
    "for combination, scores in normalized_adamic_adar_scores_two_layers.items():\n",
    "    print(f\"{combination}: {scores}\")\n",
    "\n",
    "print(\"\\nNormalized Adamic-Adar Scores for Three-Layer Combinations:\")\n",
    "for combination, scores in normalized_adamic_adar_scores_three_layers.items():\n",
    "    print(f\"{combination}: {scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with Normalized Jaccard Scores for Individual Layers:\n",
      "   Node_U  Node_V  Probability    Layer\n",
      "0   176.0   174.0          1.0  Layer_1\n",
      "1    36.0    11.0          1.0  Layer_1\n",
      "2    36.0     2.0          1.0  Layer_1\n",
      "3    11.0     2.0          1.0  Layer_1\n",
      "4    75.0    39.0          0.5  Layer_5\n",
      "\n",
      "DataFrame with Normalized Jaccard Scores for Two-Layer Combinations:\n",
      "   Node_U  Node_V  Probability               Layer\n",
      "0   176.0   174.0          1.0  (Layer_1, Layer_2)\n",
      "1    36.0    11.0          1.0  (Layer_1, Layer_2)\n",
      "2    36.0     2.0          1.0  (Layer_1, Layer_2)\n",
      "3    11.0     2.0          1.0  (Layer_1, Layer_2)\n",
      "4   176.0   174.0          1.0  (Layer_1, Layer_4)\n",
      "\n",
      "DataFrame with Normalized Jaccard Scores for Three-Layer Combinations:\n",
      "   Node_U  Node_V  Probability                        Layer\n",
      "0   176.0   174.0          1.0  (Layer_1, Layer_2, Layer_4)\n",
      "1    36.0    11.0          1.0  (Layer_1, Layer_2, Layer_4)\n",
      "2    36.0     2.0          1.0  (Layer_1, Layer_2, Layer_4)\n",
      "3    11.0     2.0          1.0  (Layer_1, Layer_2, Layer_4)\n",
      "4   176.0   174.0          1.0  (Layer_1, Layer_2, Layer_5)\n",
      "\n",
      "DataFrame with Normalized Adamic-Adar Scores for Individual Layers:\n",
      "   Node_U  Node_V  Probability    Layer\n",
      "0   176.0   174.0      1.00000  Layer_1\n",
      "1    36.0    11.0      0.63093  Layer_1\n",
      "2    36.0     2.0      0.63093  Layer_1\n",
      "3    11.0     2.0      0.63093  Layer_1\n",
      "4    75.0    39.0      1.00000  Layer_5\n",
      "\n",
      "DataFrame with Normalized Adamic-Adar Scores for Two-Layer Combinations:\n",
      "   Node_U  Node_V  Probability               Layer\n",
      "0   176.0   174.0      1.00000  (Layer_1, Layer_2)\n",
      "1    36.0    11.0      0.63093  (Layer_1, Layer_2)\n",
      "2    36.0     2.0      0.63093  (Layer_1, Layer_2)\n",
      "3    11.0     2.0      0.63093  (Layer_1, Layer_2)\n",
      "4   176.0   174.0      1.00000  (Layer_1, Layer_4)\n",
      "\n",
      "DataFrame with Normalized Adamic-Adar Scores for Three-Layer Combinations:\n",
      "   Node_U  Node_V  Probability                        Layer\n",
      "0   176.0   174.0      1.00000  (Layer_1, Layer_2, Layer_4)\n",
      "1    36.0    11.0      0.63093  (Layer_1, Layer_2, Layer_4)\n",
      "2    36.0     2.0      0.63093  (Layer_1, Layer_2, Layer_4)\n",
      "3    11.0     2.0      0.63093  (Layer_1, Layer_2, Layer_4)\n",
      "4   176.0   174.0      1.00000  (Layer_1, Layer_2, Layer_5)\n"
     ]
    }
   ],
   "source": [
    "def scores_to_dataframe(normalized_scores, layer_type):\n",
    "    links = []\n",
    "    for layer, layer_scores in normalized_scores.items():\n",
    "        for link, score in layer_scores.items():\n",
    "            node_u, node_v = map(float, link.split(' - '))\n",
    "            if layer_type == \"individual\":\n",
    "                layer_label = layer\n",
    "            else:\n",
    "                # Format the layer label correctly for combinations\n",
    "                layer_label = tuple(layer)\n",
    "            links.append({'Node_U': node_u, 'Node_V': node_v, 'Probability': score, 'Layer': layer_label})\n",
    "    return pd.DataFrame(links)\n",
    "\n",
    "# Convert normalized scores to DataFrames for individual layers, two-layer combinations, and three-layer combinations\n",
    "df_normalized_jaccard_individual = scores_to_dataframe(normalized_jaccard_scores_individual_layers, \"individual\")\n",
    "df_normalized_jaccard_two_layers = scores_to_dataframe(normalized_jaccard_scores_two_layers, \"combination\")\n",
    "df_normalized_jaccard_three_layers = scores_to_dataframe(normalized_jaccard_scores_three_layers, \"combination\")\n",
    "\n",
    "df_normalized_adamic_adar_individual = scores_to_dataframe(normalized_adamic_adar_scores_individual_layers, \"individual\")\n",
    "df_normalized_adamic_adar_two_layers = scores_to_dataframe(normalized_adamic_adar_scores_two_layers, \"combination\")\n",
    "df_normalized_adamic_adar_three_layers = scores_to_dataframe(normalized_adamic_adar_scores_three_layers, \"combination\")\n",
    "# Display the first few rows of each DataFrame\n",
    "print(\"DataFrame with Normalized Jaccard Scores for Individual Layers:\")\n",
    "print(df_normalized_jaccard_individual.head())\n",
    "\n",
    "print(\"\\nDataFrame with Normalized Jaccard Scores for Two-Layer Combinations:\")\n",
    "print(df_normalized_jaccard_two_layers.head())\n",
    "\n",
    "print(\"\\nDataFrame with Normalized Jaccard Scores for Three-Layer Combinations:\")\n",
    "print(df_normalized_jaccard_three_layers.head())\n",
    "\n",
    "print(\"\\nDataFrame with Normalized Adamic-Adar Scores for Individual Layers:\")\n",
    "print(df_normalized_adamic_adar_individual.head())\n",
    "\n",
    "print(\"\\nDataFrame with Normalized Adamic-Adar Scores for Two-Layer Combinations:\")\n",
    "print(df_normalized_adamic_adar_two_layers.head())\n",
    "\n",
    "print(\"\\nDataFrame with Normalized Adamic-Adar Scores for Three-Layer Combinations:\")\n",
    "print(df_normalized_adamic_adar_three_layers.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_flow(node, exclusive_neighbors, layer_data):\n",
    "    total_flow = 0\n",
    "    node_str = str(node)  # Convert node ID to string for key lookup\n",
    "\n",
    "    # Check if the node has exclusive neighbors in the given layer\n",
    "    if node_str not in exclusive_neighbors:\n",
    "        return 0  # Return 0 if node has no exclusive neighbors in this layer\n",
    "\n",
    "    neighbor_count = len(exclusive_neighbors[node_str])\n",
    "    for neighbor_str in exclusive_neighbors[node_str]:\n",
    "        neighbor = float(neighbor_str)  # Convert neighbor ID back to float for comparison\n",
    "\n",
    "        # Extract flow data for edges between the node and its neighbors\n",
    "        flow = layer_data.loc[((layer_data['From_Node'] == node) & (layer_data['To_Node'] == neighbor)) |\n",
    "                              ((layer_data['From_Node'] == neighbor) & (layer_data['To_Node'] == node)), 'Flow']\n",
    "\n",
    "        if not flow.empty:\n",
    "            total_flow += flow.iloc[0]  # Add the flow value to the total\n",
    "\n",
    "    # Calculate average flow if there are neighbors, otherwise return 0\n",
    "    return total_flow / neighbor_count if neighbor_count > 0 else 0\n",
    "\n",
    "def calculate_edge_weight(row, exclusive_neighbors, filtered_cases_data, layer_type):\n",
    "    if layer_type == \"individual\":\n",
    "        layer = row['Layer']\n",
    "        layer_index = int(layer.split('_')[1]) - 1\n",
    "        layer_data = filtered_cases_data[layer_index]\n",
    "        final_avg_flow_u = calculate_average_flow(row['Node_U'], exclusive_neighbors, layer_data)\n",
    "        final_avg_flow_v = calculate_average_flow(row['Node_V'], exclusive_neighbors, layer_data)\n",
    "    else:\n",
    "        # Handle layer combinations directly as a tuple\n",
    "        layers = row['Layer']\n",
    "        avg_flows_u = []\n",
    "        avg_flows_v = []\n",
    "\n",
    "        for layer in layers:\n",
    "            layer_index = int(layer.split('_')[1]) - 1\n",
    "            layer_data = filtered_cases_data[layer_index]\n",
    "\n",
    "            avg_flow_u = calculate_average_flow(row['Node_U'], exclusive_neighbors, layer_data)\n",
    "            avg_flow_v = calculate_average_flow(row['Node_V'], exclusive_neighbors, layer_data)\n",
    "\n",
    "            if avg_flow_u is not None:\n",
    "                avg_flows_u.append(avg_flow_u)\n",
    "            if avg_flow_v is not None:\n",
    "                avg_flows_v.append(avg_flow_v)\n",
    "\n",
    "        final_avg_flow_u = sum(avg_flows_u) / len(avg_flows_u) if avg_flows_u else 0\n",
    "        final_avg_flow_v = sum(avg_flows_v) / len(avg_flows_v) if avg_flows_v else 0\n",
    "\n",
    "    final_weight = (final_avg_flow_u + final_avg_flow_v) / 2 * row['Probability']\n",
    "    return final_weight\n",
    "\n",
    "def find_node_layer(node, exclusive_neighbors):\n",
    "    # Adjust the logic to identify the layer(s) a node belongs to\n",
    "    layers = []\n",
    "    for layer, neighbors in exclusive_neighbors.items():\n",
    "        if str(node) in neighbors:\n",
    "            layers.append(layer)\n",
    "    return layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to apply the calculate_edge_weight function to the DataFrames\n",
    "# Assuming df_normalized_jaccard_individual is your DataFrame for individual layers\n",
    "#df_normalized_jaccard_individual['weight'] = df_normalized_jaccard_individual.apply(\n",
    " #   lambda row: calculate_edge_weight(row, \n",
    " #                                     exclusive_neighbors_individual_layers[row['Layer']], \n",
    " #                                     filtered_cases_data, \n",
    " #                                     \"individual\"),\n",
    "  #  axis=1\n",
    "#)\n",
    "# Now apply the calculate_edge_weight function\n",
    "#df_normalized_jaccard_three_layers['weight'] = df_normalized_jaccard_three_layers.apply(\n",
    " #   lambda row: calculate_edge_weight(row,\n",
    " #                                     exclusive_neighbors_three_layers[row['Layer']],\n",
    "  #                                    filtered_cases_data,\n",
    "  #                                    \"combination\"),\n",
    "   # axis=1\n",
    "#)\n",
    "#print(\"\\nDataFrame with Normalized Jaccard Scores for Three-Layer Combinations:\")\n",
    "#print(df_normalized_jaccard_three_layers.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/jaccard_individual.csv',\n",
       " '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/jaccard_two_layers.csv',\n",
       " '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/jaccard_three_layers.csv',\n",
       " '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/adamic_adar_individual.csv',\n",
       " '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/adamic_adar_two_layers.csv',\n",
       " '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/adamic_adar_three_layers.csv']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_and_format_df(df, exclusive_neighbors, filtered_cases_data, layer_type):\n",
    "    # Apply weight calculation\n",
    "    df['weight'] = df.apply(\n",
    "        lambda row: calculate_edge_weight(row, exclusive_neighbors[row['Layer']], filtered_cases_data, layer_type),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Format for export\n",
    "    formatted_df = df[['Node_U', 'Node_V', 'Layer', 'weight']].copy()\n",
    "    if layer_type != \"individual\":\n",
    "        # For layer combinations, we need to add this link to all layers in the combination\n",
    "        combined_dfs = []\n",
    "        for index, row in formatted_df.iterrows():\n",
    "            for layer in row['Layer']:\n",
    "                combined_dfs.append({'Node_U': row['Node_U'], 'Node_V': row['Node_V'], 'Layer': layer, 'weight': row['weight']})\n",
    "        formatted_df = pd.DataFrame(combined_dfs)\n",
    "    \n",
    "    return formatted_df\n",
    "\n",
    "# Apply the function to all the DataFrames\n",
    "df_jaccard_individual_processed = process_and_format_df(df_normalized_jaccard_individual, exclusive_neighbors_individual_layers, filtered_cases_data, \"individual\")\n",
    "df_jaccard_two_layers_processed = process_and_format_df(df_normalized_jaccard_two_layers, exclusive_neighbors_two_layers, filtered_cases_data, \"combination\")\n",
    "df_jaccard_three_layers_processed = process_and_format_df(df_normalized_jaccard_three_layers, exclusive_neighbors_three_layers, filtered_cases_data, \"combination\")\n",
    "\n",
    "df_adamic_adar_individual_processed = process_and_format_df(df_normalized_adamic_adar_individual, exclusive_neighbors_individual_layers, filtered_cases_data, \"individual\")\n",
    "df_adamic_adar_two_layers_processed = process_and_format_df(df_normalized_adamic_adar_two_layers, exclusive_neighbors_two_layers, filtered_cases_data, \"combination\")\n",
    "df_adamic_adar_three_layers_processed = process_and_format_df(df_normalized_adamic_adar_three_layers, exclusive_neighbors_three_layers, filtered_cases_data, \"combination\")\n",
    "\n",
    "# Combine all processed DataFrames\n",
    "combined_df = pd.concat([\n",
    "    df_jaccard_individual_processed,\n",
    "    df_jaccard_two_layers_processed,\n",
    "    df_jaccard_three_layers_processed,\n",
    "    df_adamic_adar_individual_processed,\n",
    "    df_adamic_adar_two_layers_processed,\n",
    "    df_adamic_adar_three_layers_processed\n",
    "])\n",
    "\n",
    "# Export the combined DataFrame to a CSV file\n",
    "#combined_df.to_csv('/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/processed_network_data.csv', index=False)\n",
    "\n",
    "def export_processed_data(df, exclusive_neighbors, filtered_cases_data, layer_type, filename_prefix):\n",
    "    processed_df = process_and_format_df(df, exclusive_neighbors, filtered_cases_data, layer_type)\n",
    "    file_path = f'/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/{filename_prefix}_{layer_type}.csv'\n",
    "    processed_df.to_csv(file_path, index=False)\n",
    "    return file_path\n",
    "\n",
    "# Exporting each DataFrame to a separate CSV file\n",
    "file_jaccard_individual = export_processed_data(df_normalized_jaccard_individual, exclusive_neighbors_individual_layers, filtered_cases_data, \"individual\", \"jaccard\")\n",
    "file_jaccard_two_layers = export_processed_data(df_normalized_jaccard_two_layers, exclusive_neighbors_two_layers, filtered_cases_data, \"two_layers\", \"jaccard\")\n",
    "file_jaccard_three_layers = export_processed_data(df_normalized_jaccard_three_layers, exclusive_neighbors_three_layers, filtered_cases_data, \"three_layers\", \"jaccard\")\n",
    "\n",
    "file_adamic_adar_individual = export_processed_data(df_normalized_adamic_adar_individual, exclusive_neighbors_individual_layers, filtered_cases_data, \"individual\", \"adamic_adar\")\n",
    "file_adamic_adar_two_layers = export_processed_data(df_normalized_adamic_adar_two_layers, exclusive_neighbors_two_layers, filtered_cases_data, \"two_layers\", \"adamic_adar\")\n",
    "file_adamic_adar_three_layers = export_processed_data(df_normalized_adamic_adar_three_layers, exclusive_neighbors_three_layers, filtered_cases_data, \"three_layers\", \"adamic_adar\")\n",
    "\n",
    "# Returning the file paths for download\n",
    "[file_jaccard_individual, file_jaccard_two_layers, file_jaccard_three_layers, \n",
    " file_adamic_adar_individual, file_adamic_adar_two_layers, file_adamic_adar_three_layers]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
