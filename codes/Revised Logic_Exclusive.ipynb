{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell #: 1 Library Imports\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import uunet.multinet as ml\n",
    "import itertools\n",
    "import numpy as np  \n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell #: 2 File Paths\n",
    "# File paths for the Excel sheets\n",
    "file_paths = [\n",
    "             \"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Layer_1_70percent.xlsx\", \n",
    "             \"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Layer_2_70percent.xlsx\", \n",
    "             \"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Layer_3_70percent.xlsx\", \n",
    "             \"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Layer_4_70percent.xlsx\", \n",
    "             \"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Layer_5_70percent.xlsx\"\n",
    "             ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell #: 3 Constructing the Multiplex Network\n",
    "\n",
    "print(\"Initializing multiplex network...\")\n",
    "multiplex_net = ml.empty()\n",
    "\n",
    "# A variable to keep track of the layer index\n",
    "layer_index = 1\n",
    "\n",
    "# Iterate over each Excel file and add its sheets as layers to the multiplex network\n",
    "for file_path in file_paths:\n",
    "    print(f\"\\nProcessing file: {file_path}\")\n",
    "    xls = pd.ExcelFile(file_path)\n",
    "\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        unique_layer_name = f\"Layer_{layer_index}\"\n",
    "        print(f\"\\nAdding layer: {unique_layer_name}\")\n",
    "        df = pd.read_excel(xls, sheet_name)\n",
    "\n",
    "        # Debug: Print a sample of the DataFrame\n",
    "        print(f\"DataFrame sample from {unique_layer_name}:\\n\", df.head())\n",
    "\n",
    "        vertices = {'actor': list(set(df['From_Node']).union(df['To_Node'])), \n",
    "                    'layer': [unique_layer_name] * len(set(df['From_Node']).union(df['To_Node']))}\n",
    "        print(f\"Adding vertices to layer {unique_layer_name}: {vertices['actor']}\")\n",
    "\n",
    "        ml.add_vertices(multiplex_net, vertices)\n",
    "\n",
    "        edges = {\n",
    "            'from_actor': df['From_Node'].tolist(),\n",
    "            'from_layer': [unique_layer_name] * len(df),\n",
    "            'to_actor': df['To_Node'].tolist(),\n",
    "            'to_layer': [unique_layer_name] * len(df)\n",
    "        }\n",
    "        print(f\"Adding {len(df)} edges to layer {unique_layer_name}\")\n",
    "        print(f\"Sample edges for layer {unique_layer_name}: {edges['from_actor'][:5]} -> {edges['to_actor'][:5]}\")\n",
    "\n",
    "        ml.add_edges(multiplex_net, edges)\n",
    "\n",
    "        layer_index += 1\n",
    "\n",
    "print(\"\\nNetwork construction completed.\")\n",
    "print(f\"Total layers in network: {len(ml.layers(multiplex_net))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding exclusive neighbors for layer Layer_2\n",
      "\n",
      "Finding exclusive neighbors for layer Layer_1\n",
      "\n",
      "Finding exclusive neighbors for layer Layer_4\n",
      "\n",
      "Finding exclusive neighbors for layer Layer_5\n",
      "\n",
      "Finding exclusive neighbors for layer Layer_3\n",
      "Layer Layer_2:\n",
      "Actor 76: Exclusive neighbors: {'78', '79'}\n",
      "Actor 78: Exclusive neighbors: {'76'}\n",
      "Actor 127: Exclusive neighbors: {'117'}\n",
      "Actor 79: Exclusive neighbors: {'76'}\n",
      "Actor 117: Exclusive neighbors: {'127'}\n",
      "\n",
      "\n",
      "Layer Layer_1:\n",
      "Actor 6: Exclusive neighbors: {'36'}\n",
      "Actor 36: Exclusive neighbors: {'6'}\n",
      "Actor 175: Exclusive neighbors: {'191'}\n",
      "Actor 191: Exclusive neighbors: {'175', '202'}\n",
      "Actor 202: Exclusive neighbors: {'191'}\n",
      "\n",
      "\n",
      "Layer Layer_4:\n",
      "\n",
      "\n",
      "Layer Layer_5:\n",
      "Actor 130: Exclusive neighbors: {'141'}\n",
      "Actor 141: Exclusive neighbors: {'130'}\n",
      "\n",
      "\n",
      "Layer Layer_3:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell #: 4 Finding the Exclusive Neighbors\n",
    "# Retrieve the exclusive neighbors of all actors in all Layers\n",
    "all_layers = ml.layers(multiplex_net)\n",
    "\n",
    "exclusive_neighbors = {}\n",
    "\n",
    "for layer in all_layers:\n",
    "    print(f\"\\nFinding exclusive neighbors for layer {layer}\")\n",
    "\n",
    "    # Retrieve all vertices in the current layer\n",
    "    vertices_info = ml.vertices(multiplex_net, [layer])\n",
    "    actors = vertices_info['actor']  # List of actor IDs in the layer\n",
    "\n",
    "    # Prepare to store exclusive neighbors for the current layer\n",
    "    exclusive_neighbors[layer] = {}\n",
    "\n",
    "    # Find exclusive neighbors for each actor in the layer\n",
    "    for actor_id in actors:\n",
    "        # Retrieve exclusive neighbors for the actor\n",
    "        neighbors = ml.xneighbors(multiplex_net, actor_id, layers=[layer], mode='all')\n",
    "        exclusive_neighbors[layer][actor_id] = neighbors\n",
    "\n",
    "# Commented out file saving part\n",
    "# file_path = '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/exclusive_neighbors_Indivisual_layers70.txt'\n",
    "# with open(file_path, 'w') as file:\n",
    "#     for layer, neighbors_dict in exclusive_neighbors.items():\n",
    "#         file.write(f\"Layer {layer}:\\n\")\n",
    "#         for actor_id, neighbors in neighbors_dict.items():\n",
    "#             if neighbors:  # Only write if the actor has exclusive neighbors\n",
    "#                 file.write(f\"Actor {actor_id}: Exclusive neighbors: {neighbors}\\n\")\n",
    "#         file.write(\"\\n\")\n",
    "\n",
    "# Display the exclusive neighbors data\n",
    "for layer, neighbors_dict in exclusive_neighbors.items():\n",
    "    print(f\"Layer {layer}:\")\n",
    "    for actor_id, neighbors in neighbors_dict.items():\n",
    "        if neighbors:  # Only display if the actor has exclusive neighbors\n",
    "            print(f\"Actor {actor_id}: Exclusive neighbors: {neighbors}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link predictions for Layer_2:\n",
      "79 - 78: 1.0\n",
      "Link predictions for Layer_1:\n",
      "175 - 202: 1.0\n",
      "Link predictions for Layer_4:\n",
      "Link predictions for Layer_5:\n",
      "Link predictions for Layer_3:\n"
     ]
    }
   ],
   "source": [
    "# Cell #: 5 Link Prediction : Through Modified Jaccard Coefficient\n",
    "# Convert the multiplex network into NetworkX graphs for each layer\n",
    "nx_graphs = ml.to_nx_dict(multiplex_net)\n",
    "\n",
    "# Define the modified Jaccard coefficient function\n",
    "def modified_jaccard_coefficient(G, exclusive_neighbors):\n",
    "    def predict(u, v):\n",
    "        neighbors_u = exclusive_neighbors.get(u, set())\n",
    "        neighbors_v = exclusive_neighbors.get(v, set())\n",
    "        union_size = len(neighbors_u | neighbors_v)\n",
    "        if union_size == 0:\n",
    "            return 0\n",
    "        intersection_size = len(neighbors_u & neighbors_v)\n",
    "        return intersection_size / union_size\n",
    "\n",
    "    return ((u, v, predict(u, v)) for u, v in nx.non_edges(G))\n",
    "\n",
    "# Iterate over each layer and apply the modified Jaccard coefficient function\n",
    "for layer, exclusive_neighbors in exclusive_neighbors.items():\n",
    "    if exclusive_neighbors:  # Check if the layer has exclusive neighbors\n",
    "        G = nx_graphs[layer]\n",
    "        link_predictions = list(modified_jaccard_coefficient(G, exclusive_neighbors))\n",
    "\n",
    "        # Print link predictions for the current layer\n",
    "        print(f\"Link predictions for {layer}:\")\n",
    "        for u, v, score in link_predictions:\n",
    "            if score > 0:  # Optionally filter out zero scores\n",
    "                print(f\"{u} - {v}: {score}\")\n",
    "    else:\n",
    "        print(f\"No exclusive neighbors for {layer}, skipping link prediction.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell #: 6 Link Prediction : Through Modified Adamic-Adar Index\n",
    "\n",
    "\n",
    "# Define the file path and file name\n",
    "file_path = \"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Adamic-Adar-Score_Step01.txt\"\n",
    "\n",
    "# Assuming nx_graphs is a dictionary containing NetworkX graphs for each layer\n",
    "\n",
    "# Exclusive neighbors data for Layer_1\n",
    "exclusive_neighbors_layer_1 = {\n",
    "    '6': {'11', '2', '36'},\n",
    "    '11': {'6'},\n",
    "    '13': {'30'},\n",
    "    '2': {'6'},\n",
    "    '23': {'34'},\n",
    "    '30': {'13'},\n",
    "    '34': {'23'},\n",
    "    '36': {'6'},\n",
    "    '160': {'176', '174'},\n",
    "    '174': {'160'},\n",
    "    '175': {'191'},\n",
    "    '176': {'160'},\n",
    "    '191': {'175'},\n",
    "    '202': {'211'},\n",
    "    '211': {'212', '202'},\n",
    "    '212': {'211'}\n",
    "}\n",
    "\n",
    "# Exclusive neighbors data for Layer_5\n",
    "exclusive_neighbors_layer_5 = {\n",
    "    '26': {'37'},\n",
    "    '37': {'11', '26', '33'},\n",
    "    '48': {'28'},\n",
    "    '19': {'16', '21'},\n",
    "    '11': {'22', '37'},\n",
    "    '21': {'19'},\n",
    "    '22': {'11'},\n",
    "    '28': {'48'},\n",
    "    '33': {'37'},\n",
    "    '127': {'139'},\n",
    "    '139': {'127'},\n",
    "    '16': {'19'}\n",
    "}\n",
    "\n",
    "# Define the modified Adamic-Adar index function\n",
    "def modified_adamic_adar_index(G, exclusive_neighbors):\n",
    "    def predict(u, v):\n",
    "        common_neighbors = exclusive_neighbors.get(u, set()) & exclusive_neighbors.get(v, set())\n",
    "        return sum(1 / log(len(exclusive_neighbors.get(w, set()))) for w in common_neighbors)\n",
    "\n",
    "    return ((u, v, predict(u, v)) for u, v in nx.non_edges(G))\n",
    "\n",
    "# Apply the modified Adamic-Adar index function to Layer_1\n",
    "G_layer_1 = nx_graphs['Layer_1']\n",
    "adamic_adar_predictions_layer_1 = list(modified_adamic_adar_index(G_layer_1, exclusive_neighbors_layer_1))\n",
    "\n",
    "# Print Adamic-Adar predictions for Layer_1\n",
    "print(\"Adamic-Adar predictions for Layer_1:\")\n",
    "for u, v, score in adamic_adar_predictions_layer_1:\n",
    "    if score > 0:  # Optionally filter out zero scores\n",
    "        print(f\"{u} - {v}: {score}\")\n",
    "\n",
    "# Apply the modified Adamic-Adar index function to Layer_5\n",
    "G_layer_5 = nx_graphs['Layer_5']\n",
    "adamic_adar_predictions_layer_5 = list(modified_adamic_adar_index(G_layer_5, exclusive_neighbors_layer_5))\n",
    "\n",
    "# Print Adamic-Adar predictions for Layer_5\n",
    "print(\"Adamic-Adar predictions for Layer_5:\")\n",
    "for u, v, score in adamic_adar_predictions_layer_5:\n",
    "    if score > 0:  # Optionally filter out zero scores\n",
    "        print(f\"{u} - {v}: {score}\")\n",
    "# Save the content to the file\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(content)\n",
    "\n",
    "print(f\"Results saved to {file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell #: Save the results to a file Jaccard Coefficient\n",
    "# Define the file path and file name\n",
    "file_path = \"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Jaccard_Score_Step01.txt\"\n",
    "\n",
    "# Prepare the content to be written to the file\n",
    "content = \"Link predictions for Layer_1:\\n\"\n",
    "for u, v, score in link_predictions_layer_1:\n",
    "    if score > 0:  # Optionally filter out zero scores\n",
    "        content += f\"{u} - {v}: {score}\\n\"\n",
    "\n",
    "content += \"\\nLink predictions for Layer_5:\\n\"\n",
    "for u, v, score in link_predictions_layer_5:\n",
    "    if score > 0:  # Optionally filter out zero scores\n",
    "        content += f\"{u} - {v}: {score}\\n\"\n",
    "\n",
    "# Save the content to the file\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(content)\n",
    "\n",
    "print(f\"Results saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges in Layer 1: [('38', '66'), ('10', '56'), ('14', '3'), ('19', '67'), ('51', '73'), ('31', '50'), ('3', '52'), ('1', '47'), ('6', '2'), ('15', '22'), ('58', '59'), ('58', '74'), ('4', '42'), ('20', '28'), ('42', '25'), ('43', '17'), ('17', '75'), ('3', '34'), ('13', '30'), ('34', '23'), ('15', '24'), ('39', '28'), ('43', '15'), ('21', '23'), ('8', '35'), ('11', '1'), ('6', '11'), ('52', '72'), ('24', '25'), ('4', '22'), ('14', '71'), ('11', '17'), ('57', '58'), ('33', '35'), ('5', '33'), ('11', '62'), ('28', '62'), ('39', '17'), ('9', '57'), ('5', '35'), ('9', '34'), ('7', '33'), ('19', '20'), ('164', '178'), ('7', '11'), ('27', '11'), ('32', '21'), ('54', '11'), ('53', '11'), ('6', '36'), ('5', '32'), ('68', '69'), ('123', '115'), ('123', '114'), ('118', '110'), ('110', '119'), ('119', '128'), ('137', '125'), ('140', '137'), ('128', '140'), ('125', '124'), ('135', '147'), ('124', '135'), ('183', '181'), ('147', '157'), ('202', '189'), ('168', '183'), ('189', '161'), ('160', '149'), ('105', '111'), ('147', '156'), ('95', '100'), ('112', '105'), ('100', '106'), ('106', '113'), ('148', '158'), ('122', '112'), ('187', '199'), ('100', '105'), ('139', '151'), ('139', '150'), ('152', '163'), ('164', '179'), ('211', '212'), ('117', '126'), ('152', '164'), ('139', '152'), ('197', '209'), ('211', '215'), ('212', '216'), ('202', '211'), ('155', '167'), ('127', '138'), ('95', '101'), ('78', '81'), ('81', '85'), ('85', '90'), ('166', '182'), ('117', '127'), ('76', '78'), ('79', '76'), ('192', '175'), ('205', '192'), ('82', '86'), ('93', '98'), ('104', '109'), ('109', '117'), ('131', '143'), ('88', '93'), ('98', '104'), ('156', '169'), ('131', '122'), ('204', '213'), ('160', '175'), ('111', '120'), ('188', '201'), ('199', '210'), ('142', '131'), ('91', '95'), ('165', '154'), ('180', '165'), ('194', '180'), ('207', '194'), ('206', '214'), ('214', '207'), ('166', '154'), ('155', '146'), ('191', '202'), ('174', '160'), ('181', '166'), ('175', '191'), ('176', '160'), ('133', '123'), ('184', '168'), ('149', '162'), ('177', '193'), ('193', '206'), ('162', '177'), ('132', '123'), ('144', '132'), ('134', '123'), ('145', '134'), ('154', '145'), ('142', '133'), ('114', '107'), ('107', '115'), ('118', '125'), ('144', '142'), ('115', '118')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### Debuging Cell ####\n",
    "# Extract all layers from the multiplex network\n",
    "all_layers = ml.layers(multiplex_net)\n",
    "\n",
    "# Check if Layer 1 is present in the network\n",
    "if 'Layer_1' in all_layers:\n",
    "    # Retrieve all actors (nodes) in Layer 1\n",
    "    actors_layer_1 = ml.actors(multiplex_net, layers=['Layer_1'])\n",
    "\n",
    "    # Retrieve all edges (connections) in Layer 1\n",
    "    edges_layer_1 = ml.edges(multiplex_net, layers1=['Layer_1'], layers2=['Layer_1'])\n",
    "\n",
    "    # Format the edges data correctly using the provided keys\n",
    "    edges_data = list(zip(edges_layer_1['from_actor'], edges_layer_1['to_actor']))\n",
    "\n",
    "    # Print the formatted edges data\n",
    "    print(\"Edges in Layer 1:\", edges_data)\n",
    "else:\n",
    "    print(\"Layer_1 not found in the network.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ml.xneighbors  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     From_Node  From_Layer  To_Node  To_Layer         Flow\n",
      "88          68           1       69         1  3223.971139\n",
      "87          11           1       37         1  2808.415135\n",
      "86           5           1       32         1  2359.085320\n",
      "85          11           1       61         1  2312.460471\n",
      "84          33           1       37         1  2116.959750\n",
      "..         ...         ...      ...       ...          ...\n",
      "126        175           2      160         2    21.977866\n",
      "199        205           2      192         2    21.818687\n",
      "198        192           2      175         2    21.774148\n",
      "127        192           2      175         2    21.616584\n",
      "197        205           2      192         2    21.572046\n",
      "\n",
      "[186 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Perecentage of edges to keep Code\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def keep_top_70_percent_edges(file_path):\n",
    "    # Load the Excel file\n",
    "    data = pd.read_excel(file_path)\n",
    "\n",
    "    # Sort the edges based on 'Flow' in descending order\n",
    "    sorted_data = data.sort_values(by='Flow', ascending=False)\n",
    "\n",
    "    # Calculate the number of edges to keep (70% of the total edges)\n",
    "    number_of_edges_to_keep = int(len(sorted_data) * 0.7)\n",
    "\n",
    "    # Select the top 70% edges\n",
    "    top_70_percent_edges = sorted_data.head(number_of_edges_to_keep)\n",
    "\n",
    "    return top_70_percent_edges\n",
    "\n",
    "# Example usage for a single file\n",
    "file_path = '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Case_5.xlsx'  # Replace with your file path\n",
    "top_70_percent_edges = keep_top_70_percent_edges(file_path)\n",
    "\n",
    "# You can then view or save the result as needed\n",
    "print(top_70_percent_edges)\n",
    "top_70_percent_edges.to_excel('/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/top_70_percent_edges_sheet5.xlsx')  # To save the result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
