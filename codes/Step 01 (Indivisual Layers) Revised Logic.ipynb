{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell #: 1 Library Imports\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import uunet.multinet as ml\n",
    "import itertools\n",
    "import numpy as np  \n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell #: 2 File Paths\n",
    "# File paths for the Excel sheets\n",
    "file_paths = [\n",
    "    \"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Case_1.xlsx\",\n",
    "    \"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Case_2.xlsx\",\n",
    "    \"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Case_3.xlsx\",\n",
    "    \"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Case_4.xlsx\",\n",
    "    \"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Case_5.xlsx\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1 - Removed Edges:\n",
      "      From_Node  From_Layer  To_Node  To_Layer        Flow\n",
      "11         2.0         1.0     12.0       1.0   39.476633\n",
      "16        59.0         1.0     70.0       1.0   65.473726\n",
      "17        12.0         1.0     30.0       1.0   77.406378\n",
      "18        44.0         1.0     30.0       1.0  102.065698\n",
      "19        18.0         1.0     44.0       1.0  111.489735\n",
      "..         ...         ...      ...       ...         ...\n",
      "241      148.0         2.0    159.0       2.0  113.276302\n",
      "242      159.0         2.0    161.0       2.0  113.276302\n",
      "244      125.0         2.0    124.0       2.0  114.968040\n",
      "246      124.0         2.0    136.0       2.0  129.942968\n",
      "247      136.0         2.0    148.0       2.0  129.942968\n",
      "\n",
      "[66 rows x 5 columns] \n",
      "\n",
      "Case 2 - Removed Edges:\n",
      "      From_Node  From_Layer  To_Node  To_Layer       Flow\n",
      "0          3.0         1.0     34.0       1.0   0.000000\n",
      "1          3.0         1.0     34.0       1.0   0.000000\n",
      "2          6.0         1.0     36.0       1.0   0.000000\n",
      "3          9.0         1.0     34.0       1.0   0.000000\n",
      "5         14.0         1.0     71.0       1.0   0.000000\n",
      "..         ...         ...      ...       ...        ...\n",
      "213      200.0         2.0    199.0       2.0  27.551519\n",
      "217      133.0         2.0    130.0       2.0  28.352755\n",
      "218      133.0         2.0    130.0       2.0  28.576103\n",
      "227      174.0         2.0    160.0       2.0  52.799234\n",
      "228      176.0         2.0    160.0       2.0  54.536341\n",
      "\n",
      "[68 rows x 5 columns] \n",
      "\n",
      "Case 3 - Removed Edges:\n",
      "      From_Node  From_Layer  To_Node  To_Layer        Flow\n",
      "4          6.0         1.0     36.0       1.0    0.844136\n",
      "16        59.0         1.0     70.0       1.0   65.473726\n",
      "17        59.0         1.0     55.0       1.0  139.483920\n",
      "19         6.0         1.0      2.0       1.0  160.773154\n",
      "29        13.0         1.0     30.0       1.0  234.179626\n",
      "..         ...         ...      ...       ...         ...\n",
      "257      114.0         2.0    107.0       2.0  177.662455\n",
      "258      107.0         2.0    115.0       2.0  177.662455\n",
      "259      118.0         2.0    125.0       2.0  191.271685\n",
      "261      144.0         2.0    142.0       2.0  209.803391\n",
      "266      115.0         2.0    118.0       2.0  355.549341\n",
      "\n",
      "[63 rows x 5 columns] \n",
      "\n",
      "Case 4 - Removed Edges:\n",
      "      From_Node  From_Layer  To_Node  To_Layer        Flow\n",
      "0          3.0         1.0     34.0       1.0    0.000000\n",
      "1          3.0         1.0     34.0       1.0    0.000000\n",
      "2          6.0         1.0     36.0       1.0    0.000000\n",
      "3          9.0         1.0     34.0       1.0    0.000000\n",
      "7         14.0         1.0     71.0       1.0    0.000000\n",
      "..         ...         ...      ...       ...         ...\n",
      "257      114.0         2.0    107.0       2.0  177.662455\n",
      "258      107.0         2.0    115.0       2.0  177.662455\n",
      "259      118.0         2.0    125.0       2.0  191.271685\n",
      "261      144.0         2.0    142.0       2.0  209.803391\n",
      "266      115.0         2.0    118.0       2.0  355.549341\n",
      "\n",
      "[80 rows x 5 columns] \n",
      "\n",
      "Case 5 - Removed Edges:\n",
      "      From_Node  From_Layer  To_Node  To_Layer        Flow\n",
      "0          6.0         1.0     36.0       1.0    0.000000\n",
      "1          7.0         1.0     33.0       1.0    0.000000\n",
      "2          7.0         1.0     11.0       1.0    0.000000\n",
      "18        59.0         1.0     70.0       1.0   65.473726\n",
      "19        59.0         1.0     55.0       1.0  139.483920\n",
      "..         ...         ...      ...       ...         ...\n",
      "257      144.0         2.0    142.0       2.0  153.527609\n",
      "258      134.0         2.0    123.0       2.0  157.567615\n",
      "259      145.0         2.0    134.0       2.0  157.567615\n",
      "260      154.0         2.0    145.0       2.0  157.567615\n",
      "262      115.0         2.0    118.0       2.0  227.879123\n",
      "\n",
      "[62 rows x 5 columns] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell #: 3 Network Triming Logic\n",
    "# Reading the Excel files\n",
    "cases_data = [pd.read_excel(file_path) for file_path in file_paths]\n",
    "\n",
    "# Function to apply the filtering logic and track removed edges\n",
    "def filter_edges_and_save_removed(cases):\n",
    "    max_flow_per_edge = {}\n",
    "    filtered_cases = []\n",
    "    removed_edges = []\n",
    "\n",
    "    # Step 1: Determine the maximum flow for each edge across all cases\n",
    "    for case in cases:\n",
    "        for _, row in case.iterrows():\n",
    "            edge = (row['From_Node'], row['To_Node'])\n",
    "            flow = row['Flow']\n",
    "            max_flow_per_edge[edge] = max(max_flow_per_edge.get(edge, 0), flow)\n",
    "\n",
    "    # Steps 2 and 3: Filtering edges and tracking removed edges in each case\n",
    "    for case in cases:\n",
    "        filtered_rows = []\n",
    "        removed_edges_rows = []\n",
    "        for _, row in case.iterrows():\n",
    "            edge = (row['From_Node'], row['To_Node'])\n",
    "            if row['Flow'] >= max_flow_per_edge[edge] * 0.9:\n",
    "                filtered_rows.append(row)\n",
    "            else:\n",
    "                removed_edges_rows.append(row)\n",
    "        filtered_cases.append(pd.DataFrame(filtered_rows))\n",
    "        removed_edges.append(pd.DataFrame(removed_edges_rows))\n",
    "\n",
    "    return filtered_cases, removed_edges\n",
    "\n",
    "# Apply the filtering logic and get removed edges\n",
    "filtered_cases_data, removed_edges_data = filter_edges_and_save_removed(cases_data)\n",
    "\n",
    "# Print the results and optionally save them to files\n",
    "for i, (filtered_case, removed_edges_case) in enumerate(zip(filtered_cases_data, removed_edges_data), 1):\n",
    "    #print(f\"Case {i} - Filtered Data:\\n\", filtered_case.head())\n",
    "    print(f\"Case {i} - Removed Edges:\\n\", removed_edges_case, \"\\n\")\n",
    "    # Optionally, save the filtered cases to files\n",
    "    filtered_case.to_excel(f\"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Filtered_Case_{i}.xlsx\", index=False)\n",
    "    \n",
    "    # Optionally, save the removed edges to files\n",
    "    #removed_edges_case.to_excel(f\"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/Removed_Edges_Case_{i}.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell #: 4 Multiplex Network Construction Logic\n",
    "print(\"Initializing multiplex network...\")\n",
    "multiplex_net = ml.empty()\n",
    "\n",
    "# A variable to keep track of the layer index\n",
    "layer_index = 1\n",
    "\n",
    "# Iterate over the filtered data and add each as a layer to the multiplex network\n",
    "for df in filtered_cases_data:\n",
    "    unique_layer_name = f\"Layer_{layer_index}\"\n",
    "    print(f\"\\nAdding layer: {unique_layer_name}\")\n",
    "\n",
    "    # Debug: Print a sample of the DataFrame\n",
    "    print(f\"DataFrame sample from {unique_layer_name}:\\n\", df.head())\n",
    "\n",
    "    # Prepare vertices and add them to the network\n",
    "    vertices = {'actor': list(set(df['From_Node']).union(df['To_Node'])), \n",
    "                'layer': [unique_layer_name] * len(set(df['From_Node']).union(df['To_Node']))}\n",
    "    print(f\"Adding vertices to layer {unique_layer_name}: {vertices['actor']}\")\n",
    "    ml.add_vertices(multiplex_net, vertices)\n",
    "\n",
    "    # Prepare edges and add them to the network\n",
    "    edges = {\n",
    "        'from_actor': df['From_Node'].tolist(),\n",
    "        'from_layer': [unique_layer_name] * len(df),\n",
    "        'to_actor': df['To_Node'].tolist(),\n",
    "        'to_layer': [unique_layer_name] * len(df)\n",
    "    }\n",
    "    print(f\"Adding {len(df)} edges to layer {unique_layer_name}\")\n",
    "    print(f\"Sample edges for layer {unique_layer_name}: {edges['from_actor'][:5]} -> {edges['to_actor'][:5]}\")\n",
    "    ml.add_edges(multiplex_net, edges)\n",
    "\n",
    "    layer_index += 1\n",
    "\n",
    "print(\"\\nNetwork construction completed.\")\n",
    "print(f\"Total layers in network: {len(ml.layers(multiplex_net))}\")\n",
    "\n",
    "print(\"\\nNetwork summary:\")\n",
    "print(ml.summary(multiplex_net))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding exclusive neighbors for layer Layer_1\n",
      "\n",
      "Finding exclusive neighbors for layer Layer_4\n",
      "\n",
      "Finding exclusive neighbors for layer Layer_5\n",
      "\n",
      "Finding exclusive neighbors for layer Layer_2\n",
      "\n",
      "Finding exclusive neighbors for layer Layer_3\n",
      "Layer Layer_1:\n",
      "Actor 2.0: Exclusive neighbors: {'6.0'}\n",
      "Actor 6.0: Exclusive neighbors: {'36.0', '2.0', '11.0'}\n",
      "Actor 11.0: Exclusive neighbors: {'6.0'}\n",
      "Actor 13.0: Exclusive neighbors: {'30.0'}\n",
      "Actor 23.0: Exclusive neighbors: {'34.0'}\n",
      "Actor 30.0: Exclusive neighbors: {'13.0'}\n",
      "Actor 34.0: Exclusive neighbors: {'23.0'}\n",
      "Actor 36.0: Exclusive neighbors: {'6.0'}\n",
      "Actor 160.0: Exclusive neighbors: {'174.0', '176.0'}\n",
      "Actor 174.0: Exclusive neighbors: {'160.0'}\n",
      "Actor 175.0: Exclusive neighbors: {'191.0'}\n",
      "Actor 176.0: Exclusive neighbors: {'160.0'}\n",
      "Actor 191.0: Exclusive neighbors: {'175.0'}\n",
      "\n",
      "\n",
      "Layer Layer_4:\n",
      "\n",
      "\n",
      "Layer Layer_5:\n",
      "Actor 133.0: Exclusive neighbors: {'130.0'}\n",
      "Actor 11.0: Exclusive neighbors: {'37.0', '62.0'}\n",
      "Actor 15.0: Exclusive neighbors: {'43.0'}\n",
      "Actor 17.0: Exclusive neighbors: {'75.0', '39.0'}\n",
      "Actor 19.0: Exclusive neighbors: {'20.0'}\n",
      "Actor 28.0: Exclusive neighbors: {'39.0', '20.0', '62.0'}\n",
      "Actor 33.0: Exclusive neighbors: {'37.0'}\n",
      "Actor 37.0: Exclusive neighbors: {'11.0', '33.0'}\n",
      "Actor 43.0: Exclusive neighbors: {'15.0'}\n",
      "Actor 20.0: Exclusive neighbors: {'19.0', '28.0'}\n",
      "Actor 39.0: Exclusive neighbors: {'17.0', '28.0'}\n",
      "Actor 62.0: Exclusive neighbors: {'11.0', '28.0'}\n",
      "Actor 75.0: Exclusive neighbors: {'17.0'}\n",
      "Actor 130.0: Exclusive neighbors: {'133.0', '141.0'}\n",
      "Actor 141.0: Exclusive neighbors: {'130.0'}\n",
      "\n",
      "\n",
      "Layer Layer_2:\n",
      "Actor 124.0: Exclusive neighbors: {'125.0'}\n",
      "Actor 125.0: Exclusive neighbors: {'124.0'}\n",
      "\n",
      "\n",
      "Layer Layer_3:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell #: 5 Finding the Exclusive Neighbors\n",
    "# Retrieve the exclusive neighbors of all actors in all Layers\n",
    "all_layers = ml.layers(multiplex_net)\n",
    "\n",
    "exclusive_neighbors = {}\n",
    "\n",
    "for layer in all_layers:\n",
    "    print(f\"\\nFinding exclusive neighbors for layer {layer}\")\n",
    "\n",
    "    # Retrieve all vertices in the current layer\n",
    "    vertices_info = ml.vertices(multiplex_net, [layer])\n",
    "    actors = vertices_info['actor']  # List of actor IDs in the layer\n",
    "\n",
    "    # Prepare to store exclusive neighbors for the current layer\n",
    "    exclusive_neighbors[layer] = {}\n",
    "\n",
    "    # Find exclusive neighbors for each actor in the layer\n",
    "    for actor_id in actors:\n",
    "        # Retrieve exclusive neighbors for the actor\n",
    "        neighbors = ml.xneighbors(multiplex_net, actor_id, layers=[layer], mode='all')\n",
    "        #print(f\"Type of neighbors for actor {actor_id} in layer {layer}: {type(neighbors)}\")\n",
    "        exclusive_neighbors[layer][actor_id] = neighbors\n",
    "\n",
    "\n",
    "# Commented out file saving part\n",
    "# file_path = '/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/exclusive_neighbors_Indivisual_layers70.txt'\n",
    "# with open(file_path, 'w') as file:\n",
    "#     for layer, neighbors_dict in exclusive_neighbors.items():\n",
    "#         file.write(f\"Layer {layer}:\\n\")\n",
    "#         for actor_id, neighbors in neighbors_dict.items():\n",
    "#             if neighbors:  # Only write if the actor has exclusive neighbors\n",
    "#                 file.write(f\"Actor {actor_id}: Exclusive neighbors: {neighbors}\\n\")\n",
    "#         file.write(\"\\n\")\n",
    "\n",
    "# Display the exclusive neighbors data\n",
    "for layer, neighbors_dict in exclusive_neighbors.items():\n",
    "    print(f\"Layer {layer}:\")\n",
    "    for actor_id, neighbors in neighbors_dict.items():\n",
    "        if neighbors:  # Only display if the actor has exclusive neighbors\n",
    "            print(f\"Actor {actor_id}: Exclusive neighbors: {neighbors}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link prediction for Layer_1: 176.0 - 174.0: 1.0\n",
      "Link prediction for Layer_1: 2.0 - 36.0: 1.0\n",
      "Link prediction for Layer_1: 2.0 - 11.0: 1.0\n",
      "Link prediction for Layer_1: 36.0 - 11.0: 1.0\n",
      "Link prediction for Layer_5: 17.0 - 28.0: 0.25\n",
      "Link prediction for Layer_5: 33.0 - 11.0: 0.5\n",
      "Link prediction for Layer_5: 19.0 - 28.0: 0.3333333333333333\n",
      "Link prediction for Layer_5: 37.0 - 62.0: 0.3333333333333333\n",
      "Link prediction for Layer_5: 75.0 - 39.0: 0.5\n",
      "Link prediction for Layer_5: 133.0 - 141.0: 1.0\n",
      "Link prediction for Layer_5: 62.0 - 39.0: 0.3333333333333333\n",
      "Link prediction for Layer_5: 62.0 - 20.0: 0.3333333333333333\n",
      "Link prediction for Layer_5: 39.0 - 20.0: 0.3333333333333333\n",
      "Link prediction for Layer_5: 11.0 - 28.0: 0.25\n"
     ]
    }
   ],
   "source": [
    "# Cell #: 6 Link Prediction : Through Modified Jaccard Coefficient\n",
    "# Convert the multiplex network into NetworkX graphs for each layer\n",
    "nx_graphs = ml.to_nx_dict(multiplex_net)\n",
    "\n",
    "# Define the modified Jaccard coefficient function\n",
    "def modified_jaccard_coefficient(G, exclusive_neighbors):\n",
    "    def predict(u, v):\n",
    "        neighbors_u = exclusive_neighbors.get(u, set())\n",
    "        neighbors_v = exclusive_neighbors.get(v, set())\n",
    "        union_size = len(neighbors_u | neighbors_v)\n",
    "        if union_size == 0:\n",
    "            return 0\n",
    "        intersection_size = len(neighbors_u & neighbors_v)\n",
    "        return intersection_size / union_size\n",
    "\n",
    "    return ((u, v, predict(u, v)) for u, v in nx.non_edges(G))\n",
    "\n",
    "# Calculate and store Jaccard scores\n",
    "jaccard_scores = {}\n",
    "\n",
    "for layer, layer_exclusive_neighbors in exclusive_neighbors.items():\n",
    "    if layer_exclusive_neighbors:  # Check if the layer has exclusive neighbors\n",
    "        G = nx_graphs[layer]\n",
    "        jaccard_scores[layer] = {}\n",
    "        for u, v, score in modified_jaccard_coefficient(G, layer_exclusive_neighbors):\n",
    "            if score > 0:\n",
    "                jaccard_scores[layer][f\"{u} - {v}\"] = score\n",
    "                print(f\"Link prediction for {layer}: {u} - {v}: {score}\")\n",
    "    else:\n",
    "        print(f\"No exclusive neighbors for {layer}, skipping link prediction.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adamic-Adar prediction for Layer_1: 176.0 - 174.0: 1.4426950408889634\n",
      "Adamic-Adar prediction for Layer_1: 2.0 - 36.0: 0.9102392266268373\n",
      "Adamic-Adar prediction for Layer_1: 2.0 - 11.0: 0.9102392266268373\n",
      "Adamic-Adar prediction for Layer_1: 36.0 - 11.0: 0.9102392266268373\n",
      "Adamic-Adar prediction for Layer_5: 17.0 - 28.0: 1.4426950408889634\n",
      "Adamic-Adar prediction for Layer_5: 33.0 - 11.0: 1.4426950408889634\n",
      "Adamic-Adar prediction for Layer_5: 19.0 - 28.0: 1.4426950408889634\n",
      "Adamic-Adar prediction for Layer_5: 37.0 - 62.0: 1.4426950408889634\n",
      "Adamic-Adar prediction for Layer_5: 75.0 - 39.0: 1.4426950408889634\n",
      "Adamic-Adar prediction for Layer_5: 133.0 - 141.0: 1.4426950408889634\n",
      "Adamic-Adar prediction for Layer_5: 62.0 - 39.0: 0.9102392266268373\n",
      "Adamic-Adar prediction for Layer_5: 62.0 - 20.0: 0.9102392266268373\n",
      "Adamic-Adar prediction for Layer_5: 39.0 - 20.0: 0.9102392266268373\n",
      "Adamic-Adar prediction for Layer_5: 11.0 - 28.0: 1.4426950408889634\n"
     ]
    }
   ],
   "source": [
    "# Cell #: 7 Link Prediction : Through Modified Adamic-Adar Index\n",
    "\n",
    "# Convert the multiplex network into NetworkX graphs for each layer\n",
    "nx_graphs = ml.to_nx_dict(multiplex_net)\n",
    "\n",
    "# Define the modified Adamic-Adar index function\n",
    "def modified_adamic_adar_index(G, exclusive_neighbors):\n",
    "    def predict(u, v):\n",
    "        common_neighbors = exclusive_neighbors.get(u, set()) & exclusive_neighbors.get(v, set())\n",
    "        return sum(1 / log(len(exclusive_neighbors.get(w, set()))) for w in common_neighbors)\n",
    "\n",
    "    return ((u, v, predict(u, v)) for u, v in nx.non_edges(G))\n",
    "\n",
    "# Calculate and store Adamic-Adar scores\n",
    "adamic_adar_scores = {}\n",
    "\n",
    "for layer, layer_exclusive_neighbors in exclusive_neighbors.items():\n",
    "    if layer_exclusive_neighbors:\n",
    "        G = nx_graphs[layer]\n",
    "        adamic_adar_scores[layer] = {}\n",
    "        for u, v, score in modified_adamic_adar_index(G, layer_exclusive_neighbors):\n",
    "            if score > 0:\n",
    "                adamic_adar_scores[layer][f\"{u} - {v}\"] = score\n",
    "                print(f\"Adamic-Adar prediction for {layer}: {u} - {v}: {score}\")\n",
    "    else:\n",
    "        print(f\"No exclusive neighbors for {layer}, skipping Adamic-Adar prediction.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Normalized Jaccard Scores:\n",
      "Layer Layer_1:\n",
      "176.0 - 174.0: 1.0\n",
      "2.0 - 36.0: 1.0\n",
      "2.0 - 11.0: 1.0\n",
      "36.0 - 11.0: 1.0\n",
      "\n",
      "Layer Layer_5:\n",
      "17.0 - 28.0: 0.25\n",
      "33.0 - 11.0: 0.5\n",
      "19.0 - 28.0: 0.3333333333333333\n",
      "37.0 - 62.0: 0.3333333333333333\n",
      "75.0 - 39.0: 0.5\n",
      "133.0 - 141.0: 1.0\n",
      "62.0 - 39.0: 0.3333333333333333\n",
      "62.0 - 20.0: 0.3333333333333333\n",
      "39.0 - 20.0: 0.3333333333333333\n",
      "11.0 - 28.0: 0.25\n",
      "\n",
      "All Normalized Adamic-Adar Scores:\n",
      "Layer Layer_1:\n",
      "176.0 - 174.0: 1.0\n",
      "2.0 - 36.0: 0.6309297535714574\n",
      "2.0 - 11.0: 0.6309297535714574\n",
      "36.0 - 11.0: 0.6309297535714574\n",
      "\n",
      "Layer Layer_5:\n",
      "17.0 - 28.0: 1.0\n",
      "33.0 - 11.0: 1.0\n",
      "19.0 - 28.0: 1.0\n",
      "37.0 - 62.0: 1.0\n",
      "75.0 - 39.0: 1.0\n",
      "133.0 - 141.0: 1.0\n",
      "62.0 - 39.0: 0.6309297535714574\n",
      "62.0 - 20.0: 0.6309297535714574\n",
      "39.0 - 20.0: 0.6309297535714574\n",
      "11.0 - 28.0: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell #: 8 Results Normalization Logic\n",
    "\n",
    "def normalize_by_max(scores):\n",
    "    # Flatten all scores into a single list, excluding empty layers\n",
    "    all_scores = [score for layer_scores in scores.values() if layer_scores for score in layer_scores.values()]\n",
    "    \n",
    "    # Check if there are no scores to normalize\n",
    "    if not all_scores:\n",
    "        print(\"No scores to normalize.\")\n",
    "        return {}\n",
    "    \n",
    "    # Find the maximum score for normalization\n",
    "    max_score = max(all_scores)\n",
    "    \n",
    "    # Normalize the scores\n",
    "    normalized = {}\n",
    "    for layer, layer_scores in scores.items():\n",
    "        if layer_scores:  # Check if the layer has scores\n",
    "            normalized[layer] = {link: score / max_score for link, score in layer_scores.items()}\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "# Normalize Jaccard and Adamic-Adar scores\n",
    "normalized_jaccard = normalize_by_max(jaccard_scores)\n",
    "normalized_adamic_adar = normalize_by_max(adamic_adar_scores)\n",
    "\n",
    "# Debugging: Display all normalized scores\n",
    "print(\"All Normalized Jaccard Scores:\")\n",
    "for layer, links in normalized_jaccard.items():\n",
    "    print(f\"Layer {layer}:\")\n",
    "    for link, score in links.items():\n",
    "        print(f\"{link}: {score}\")\n",
    "    print()\n",
    "\n",
    "print(\"All Normalized Adamic-Adar Scores:\")\n",
    "for layer, links in normalized_adamic_adar.items():\n",
    "    print(f\"Layer {layer}:\")\n",
    "    for link, score in links.items():\n",
    "        print(f\"{link}: {score}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Links with Normalized Jaccard Scores:\n",
      "    node_u  node_v  probability    layer\n",
      "0    176.0   174.0     1.000000  Layer_1\n",
      "1      2.0    36.0     1.000000  Layer_1\n",
      "2      2.0    11.0     1.000000  Layer_1\n",
      "3     36.0    11.0     1.000000  Layer_1\n",
      "4     17.0    28.0     0.250000  Layer_5\n",
      "5     33.0    11.0     0.500000  Layer_5\n",
      "6     19.0    28.0     0.333333  Layer_5\n",
      "7     37.0    62.0     0.333333  Layer_5\n",
      "8     75.0    39.0     0.500000  Layer_5\n",
      "9    133.0   141.0     1.000000  Layer_5\n",
      "10    62.0    39.0     0.333333  Layer_5\n",
      "11    62.0    20.0     0.333333  Layer_5\n",
      "12    39.0    20.0     0.333333  Layer_5\n",
      "13    11.0    28.0     0.250000  Layer_5\n",
      "\n",
      "Predicted Links with Normalized Adamic-Adar Scores:\n",
      "    node_u  node_v  probability    layer\n",
      "0    176.0   174.0      1.00000  Layer_1\n",
      "1      2.0    36.0      0.63093  Layer_1\n",
      "2      2.0    11.0      0.63093  Layer_1\n",
      "3     36.0    11.0      0.63093  Layer_1\n",
      "4     17.0    28.0      1.00000  Layer_5\n",
      "5     33.0    11.0      1.00000  Layer_5\n",
      "6     19.0    28.0      1.00000  Layer_5\n",
      "7     37.0    62.0      1.00000  Layer_5\n",
      "8     75.0    39.0      1.00000  Layer_5\n",
      "9    133.0   141.0      1.00000  Layer_5\n",
      "10    62.0    39.0      0.63093  Layer_5\n",
      "11    62.0    20.0      0.63093  Layer_5\n",
      "12    39.0    20.0      0.63093  Layer_5\n",
      "13    11.0    28.0      1.00000  Layer_5\n"
     ]
    }
   ],
   "source": [
    "# Cell #: 9 Dictionary to DataFrame Conversion Logic (Flow Prediction)\n",
    "def scores_to_dataframe(normalized_scores):\n",
    "    links = []\n",
    "    for layer, layer_links in normalized_scores.items():\n",
    "        for link, score in layer_links.items():\n",
    "            node_u, node_v = map(float, link.split(\" - \"))\n",
    "            links.append({'node_u': node_u, 'node_v': node_v, 'probability': score, 'layer': layer})\n",
    "    return pd.DataFrame(links)\n",
    "\n",
    "# Convert normalized scores to DataFrames\n",
    "predicted_links_jaccard = scores_to_dataframe(normalized_jaccard)\n",
    "predicted_links_adamic_adar = scores_to_dataframe(normalized_adamic_adar)\n",
    "\n",
    "# Print the DataFrames\n",
    "print(\"Predicted Links with Normalized Jaccard Scores:\")\n",
    "print(predicted_links_jaccard)  # Print the first few rows\n",
    "\n",
    "print(\"\\nPredicted Links with Normalized Adamic-Adar Scores:\")\n",
    "print(predicted_links_adamic_adar)  # Print the first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell #: 10 Calculating the Edge Weights (Flow Prediction)\n",
    "def calculate_average_flow(node, exclusive_neighbors, layer_data):\n",
    "    total_flow = 0\n",
    "    node_str = str(node)  # Convert node ID to string for key lookup\n",
    "\n",
    "    if node_str not in exclusive_neighbors:\n",
    "        return 0  # Return 0 if node is not found\n",
    "\n",
    "    neighbor_count = len(exclusive_neighbors[node_str])\n",
    "    for neighbor_str in exclusive_neighbors[node_str]:\n",
    "        neighbor = float(neighbor_str)  # Convert neighbor ID back to float for comparison\n",
    "        flow = layer_data[((layer_data['From_Node'] == node) & (layer_data['To_Node'] == neighbor)) |\n",
    "                          ((layer_data['From_Node'] == neighbor) & (layer_data['To_Node'] == node))]\n",
    "        if not flow.empty:\n",
    "            total_flow += flow.iloc[0]['Flow']\n",
    "\n",
    "    return total_flow / neighbor_count if neighbor_count > 0 else 0\n",
    "\n",
    "def calculate_edge_weight(row, exclusive_neighbors, filtered_cases_data):\n",
    "    layer = row['layer']\n",
    "    layer_data = filtered_cases_data[int(layer.split('_')[1]) - 1]\n",
    "\n",
    "    avg_flow_u = calculate_average_flow(row['node_u'], exclusive_neighbors[layer], layer_data)\n",
    "    avg_flow_v = calculate_average_flow(row['node_v'], exclusive_neighbors[layer], layer_data)\n",
    "\n",
    "    final_weight = (avg_flow_u + avg_flow_v) / 2 * row['probability']\n",
    "    return final_weight\n",
    "\n",
    "# Apply weight calculation to DataFrames\n",
    "predicted_links_jaccard['weight'] = predicted_links_jaccard.apply(lambda row: calculate_edge_weight(row, exclusive_neighbors, filtered_cases_data), axis=1)\n",
    "predicted_links_adamic_adar['weight'] = predicted_links_adamic_adar.apply(lambda row: calculate_edge_weight(row, exclusive_neighbors, filtered_cases_data), axis=1)\n",
    "\n",
    "# Filter and Print the DataFrames for links with probability >= 0.5\n",
    "print(\"Predicted Links with Jaccard Scores and Weights (Probability >= 0.5):\")\n",
    "print(predicted_links_jaccard[predicted_links_jaccard['probability'] >= 0.5])\n",
    "\n",
    "print(\"\\nPredicted Links with Adamic-Adar Scores and Weights (Probability >= 0.5):\")\n",
    "print(predicted_links_adamic_adar[predicted_links_adamic_adar['probability'] >= 0.5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Cell to save the removed edges to a file ####\n",
    "# Path to save the removed edges file\n",
    "removed_edges_file_path = \"/Volumes/Data/NDSU/PhD Work/Research/IME Research/AI-Energy/Data/Updated Data/removed_edges.txt\"\n",
    "\n",
    "# Save the removed edges to the file\n",
    "with open(removed_edges_file_path, 'w') as file:\n",
    "    for i, removed_edges_case in enumerate(removed_edges_data, 1):\n",
    "        # Write the layer information\n",
    "        file.write(f\"Case {i} - Removed Edges:\\n\")\n",
    "        \n",
    "        # Write the removed edges\n",
    "        for index, row in removed_edges_case.iterrows():\n",
    "            file.write(f\"{row['From_Node']} -> {row['To_Node']} (Flow: {row['Flow']})\\n\")\n",
    "        \n",
    "        # Write a separator between cases\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "print(f\"Removed edges saved to {removed_edges_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Debuging Cell ####\n",
    "# Extract all layers from the multiplex network\n",
    "all_layers = ml.layers(multiplex_net)\n",
    "\n",
    "# Check if Layer 1 is present in the network\n",
    "if 'Layer_1' in all_layers:\n",
    "    # Retrieve all actors (nodes) in Layer 1\n",
    "    actors_layer_1 = ml.actors(multiplex_net, layers=['Layer_1'])\n",
    "\n",
    "    # Retrieve all edges (connections) in Layer 1\n",
    "    edges_layer_1 = ml.edges(multiplex_net, layers1=['Layer_1'], layers2=['Layer_1'])\n",
    "\n",
    "    # Format the edges data correctly using the provided keys\n",
    "    edges_data = list(zip(edges_layer_1['from_actor'], edges_layer_1['to_actor']))\n",
    "\n",
    "    # Print the formatted edges data\n",
    "    print(\"Edges in Layer 1:\", edges_data)\n",
    "else:\n",
    "    print(\"Layer_1 not found in the network.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
